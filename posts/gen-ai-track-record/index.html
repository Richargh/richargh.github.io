<!DOCTYPE html>
<html lang="en">
<head>
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <meta property="og:site_name" content="Knowledge Continuum"/>
    <meta property="og:description" content="">
    <meta property="article:author" content="https://richargh.de/about/"><meta property="og:title" content="Generative AI Track record">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://richargh.de/posts/gen-ai-track-record/"><title>Richard's Blog</title>

    <link rel="canonical" href="https://richargh.de/posts/gen-ai-track-record/"/>
    <link rel="apple-touch-icon" href="/assets/img/profile.jpg">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/>

    <link rel="icon" href="/assets/img/favicon.ico" type="image/png" sizes="16x16"/>
    <link href="/assets/css/style.css" rel="stylesheet" media="all" class="default"/>

    <!--[if IE]>
        <link href="/assets/css/ie-target.css" rel="stylesheet" type="text/css"/>
    <![endif]-->
    <!--<link href="/assets/css/prism.css" rel="stylesheet" />-->
    <link rel="alternate" type="application/rss+xml" href="https://richargh.de/feed.xml">
</head>

<body>
    <div class="container">                    
        <div class = "box"><header>
    <div class="dashboard disable-select">
        <div class="site-heading profile-board-col">
           <h4 class="medium"><a href="/">Richard's Blog</a></h4>
        </div>

        <div class="userboard profile-board-col">
            <div class="username">
                <p class="title-sans">Richard Gro√ü</p>
            </div>
            <div class="userdesc">
                <p class="title-sans">IT Archaeologist</p>
            </div>
        </div>
    </div>
    <div class="avatar disable-select">
        <a class="avatar-link" href="/">
            <img src="/assets/img/profile.jpg" class="avatar-img" alt="avatar"/>
        </a>
    </div>

    <!-- Navbar -->
    <div class="main-site-subheader menu disable-select">
        <div class="talks">
            <a style="text-decoration: none;" href="/talks">
                <!-- from https://www.reshot.com/free-svg-icons/item/speaker-ZQA938GVWS/ -->
                <svg class="icon-talks" width="18" height="19" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg">
                    <path d="M15 25.983a1 1 0 0 1-.406-.086L4.218 21.29H1a1 1 0 0 1-1-1v-8.614a1 1 0 0 1 1-1h3.218L14.594 6.07A1 1 0 0 1 16 6.983v18a1 1 0 0 1-1 1zM2 19.29h2.43c.14 0 .278.029.405.085L14 23.445V8.52l-9.165 4.07a1.002 1.002 0 0 1-.405.085H2v6.614zM23.698 28a1 1 0 0 1-.39-1.92A10.942 10.942 0 0 0 30 15.982c0-4.379-2.599-8.33-6.62-10.065a1 1 0 0 1-.522-1.314.997.997 0 0 1 1.314-.522A12.95 12.95 0 0 1 32 15.982c0 5.22-3.106 9.906-7.913 11.939a1 1 0 0 1-.39.08z"/>
                    <path d="M22.751 24.051a1 1 0 0 1-.47-1.883A6.993 6.993 0 0 0 26 15.983a6.992 6.992 0 0 0-3.719-6.185 1 1 0 1 1 .939-1.766 8.988 8.988 0 0 1 4.78 7.95 8.988 8.988 0 0 1-4.78 7.952.993.993 0 0 1-.469.117z"/>
                    <path d="M19 20.733a1 1 0 0 1 0-2c1.517 0 2.75-1.233 2.75-2.75s-1.233-2.75-2.75-2.75a1 1 0 0 1 0-2c2.62 0 4.75 2.13 4.75 4.75s-2.13 4.75-4.75 4.75z"/>
                </svg>
                <p class="home-p">Talks</p>
            </a>
        </div>
        <div class="published">
            <a style="text-decoration: none;" href="/published">
                <!-- from https://www.reshot.com/free-svg-icons/item/paper-plane-D72FP5NHXZ/ -->
                <svg class="icon-published" width="25" height="25" viewBox="0 0 2048 2048" xmlns="http://www.w3.org/2000/svg">
                    <g id="Layer_x0020_1"><g id="_559822392"><path id="_559822728" class="fil0" d="M1657.53 872.581 348.07 521.715l493.348 993.204 338.593-451.457-.1-.074a31.858 31.858 0 0 1 16.456-11.467l461.159-179.34zM296.27 441.664l1470.41 393.994c10.174 2.183 19.089 9.275 23.141 19.697 6.406 16.47-1.755 35.018-18.225 41.423l-11.599-29.824 11.5 29.75-545.971 212.322-362.47 483.291a31.844 31.844 0 0 1-12.97 11.81c-15.828 7.86-35.034 1.403-42.895-14.424l28.66-14.236-28.626 14.125L260.4 488.722c-4.181-7.128-5.62-15.863-3.312-24.475 4.573-17.07 22.121-27.201 39.191-22.628l-.01.04z"/><path
                            id="_559822560" class="fil0"
                            d="M305.626 445.908c-14.705-9.733-34.518-5.705-44.251 9-9.734 14.705-5.705 34.518 9 44.251l917.512 610.055c14.705 9.734 34.518 5.705 44.251-9 9.733-14.705 5.705-34.518-9-44.251L305.626 445.908z"/><path
                            id="_559822608" class="fil0"
                            d="M1237.39 1080.09c-1.38-17.604-16.772-30.756-34.376-29.375-17.604 1.38-30.756 16.772-29.375 34.376l28.922 361.603-132.165-169.905c-10.838-13.944-30.931-16.462-44.875-5.625-13.944 10.838-16.462 30.931-5.625 44.875l197.058 253.327c6.326 8.797 16.966 14.166 28.553 13.24 17.617-1.407 30.755-16.833 29.347-34.449l-.026.003-37.437-468.07z"/></g></g>
                    <path style="fill:none" d="M0 0h2048v2048H0z"/>
                </svg>
                <p class="published-p">Published</p>
            </a>
        </div>
        <div class="built">
            <a style="text-decoration: none;" href="/built">
                <!-- from https://www.reshot.com/free-svg-icons/item/tool-UACF35PGN9/ -->
                <svg class="icon-built" width="18" height="19" viewBox="0 0 14 23" xmlns="http://www.w3.org/2000/svg" >
                    <path d="M10 23V13a7.551 7.551 0 0 0 4-6.682A6.992 6.992 0 0 0 10 0v6.318a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V0a6.992 6.992 0 0 0-4 6.318A7.551 7.551 0 0 0 4 13v10h2v-8h2v8z"/>
                </svg>
                <p class="built-p">Built</p>
            </a>
        </div>
        <div class="home">
            <a style="text-decoration: none;" href="/about">
                <svg class="icon-home" width="18" height="19" viewBox="0 0 25 25" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M20.9777 21.6138V19.6138C20.9777 18.553 20.5563 17.5356 19.8061 16.7854C19.056 16.0353 18.0386 15.6138 16.9777 15.6138H8.97768C7.91682 15.6138 6.8994 16.0353 6.14926 16.7854C5.39911 17.5356 4.97768 18.553 4.97768 19.6138V21.6138"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    <path d="M12.9777 11.6138C15.1868 11.6138 16.9777 9.82298 16.9777 7.61385C16.9777 5.40471 15.1868 3.61385 12.9777 3.61385C10.7685 3.61385 8.97768 5.40471 8.97768 7.61385C8.97768 9.82298 10.7685 11.6138 12.9777 11.6138Z" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                </svg>
                <p class="home-p">About</p>  
            </a>
        </div>
        <div class="categories">
            <a style="text-decoration: none;" href="/tags">
            <svg class="icon-category" width="18" height="19" viewBox="0 0 24 25" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M4 9.5H20"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                <path d="M4 15.5H20"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                <path d="M10 3.5L8 21.5"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                <path d="M16 3.5L14 21.5"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>
            <p class="categories-p">Tags</p>
            </a>
        </div>
        <div class="rss">
            <a style="text-decoration: none;" href="/feed.xml">
                <svg class="icon-rss" width="18" height="19" viewBox="0 0 24 25" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M4 11.5C6.38695 11.5 8.67613 12.4482 10.364 14.136C12.0518 15.8239 13 18.1131 13 20.5"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    <path d="M4 4.5C8.24346 4.5 12.3131 6.18571 15.3137 9.18629C18.3143 12.1869 20 16.2565 20 20.5"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    <path d="M5 20.5C5.55228 20.5 6 20.0523 6 19.5C6 18.9477 5.55228 18.5 5 18.5C4.44772 18.5 4 18.9477 4 19.5C4 20.0523 4.44772 20.5 5 20.5Z"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                </svg>
            <p class="rss-p">RSS</p>
            </a>
        </div>
    </div>
    <div class="searchbar search-container">
        <i class="fa fa-search" aria-hidden="true"></i>
        <label for="search-input"></label>
        <input type="text" oninput="changeResultContainerDisp(this.value)" id="search-input" autocomplete="off" placeholder="Search the Blog..."/>
        <ul id="results-container"></ul>
    </div>
    <script src="/assets/js/simple-jekyll-search.min.js"></script>
    <script>
        function changeResultContainerDisp(val) {
            if (val) {
                document.getElementById("results-container").style.display = "block";
                document.getElementById("search-input").addEventListener('blur', function() {
                    document.addEventListener('click', function(event) {
                        var isClickInside = document.getElementById("results-container").contains(event.target);
                        if (!isClickInside) {
                            document.getElementById("results-container").style.display = "none";
                        }
                    })
                })
            }  else {
                document.getElementById("results-container").style.display = "none";
            }
        }
        var sjs = SimpleJekyllSearch({
                    searchInput: document.getElementById('search-input'),
                    resultsContainer: document.getElementById('results-container'),
                    json: '/search.json',
                    searchResultTemplate: '<li class="search_res" style="list-style: none;"><a href="https://richargh.de{url}" style="text-decoration: none; color: #555555;"><p style="font-size: 1.0rem; font-family: "Inter !important"; font-weight: 600;">{title}</p></a></li>',
                    noResultsText: 'No results found',
                    fuzzy: false,
                    limit: 4
                    })
    </script><div class="main-site-subheader disable-select" id="scroll-head" onclick="window.location.assign('/');">
        <div class="back-icon">
            <svg class="ripple" xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" width="24">
                <path d="M0 0h24v24H0z" fill="none"/>
                <path d="M21 11H6.83l3.58-3.59L9 6l-6 6 6 6 1.41-1.41L6.83 13H21z"/>
            </svg>
        </div>
        <p class="back-p">Home</p>
    </div></header><main>
                <h1>Generative AI Track record</h1><!-- Parse internal links, external links, transclusions etc and manipuate the content to reflect it accordingly --><ul class="tags"><li><a href="javascript:void(0)" class="tag"><b>Status:</b> <i>Ongoing</i></a></li><li><a href="/dates/#23-October-2025" class="tag">October-23-2025</a></li>
    <!-- Loop through page categories and print them in tags --><li><a href="/tags/#genai" class="tag">genai</a></li><li><a href="/tags/#ai" class="tag">ai</a></li><li><a href="/tags/#llm" class="tag">llm</a></li></ul><div class="content"><div class="sect1">
<h2 id="general-llm-comparisons"><a class="anchor" href="#general-llm-comparisons"></a><a class="link" href="#general-llm-comparisons">General LLM comparisons</a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://arcprize.org/leaderboard">ARC-AGI Leaderboard</a>, shows cost vs score</p>
</li>
<li>
<p><a href="https://artificialanalysis.ai/">Artificial Analysis</a> of AI models and API providers</p>
</li>
<li>
<p><a href="https://www.swebench.com/#verified">SWE-bench</a>, Can Language Models Resolve Real-World GitHub Issues?</p>
</li>
<li>
<p><a href="https://bbycroft.net/llm">LLM Visualization</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-22-chatgpts-atlas-the-browser-thats-anti-web"><a class="anchor" href="#2025-10-22-chatgpts-atlas-the-browser-thats-anti-web"></a><a class="link" href="#2025-10-22-chatgpts-atlas-the-browser-thats-anti-web">2025-10-22 <a href="https://www.anildash.com//2025/10/22/atlas-anti-web-browser/">ChatGPT&#8217;s Atlas: The Browser That&#8217;s Anti-Web</a></a></h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Atlas substitutes its own AI-generated content for the web, but it looks like it&#8217;s showing you the web</p>
</li>
<li>
<p>The user experience makes you guess what commands to type instead of clicking on links</p>
</li>
<li>
<p>You&#8217;re the agent for the browser, it&#8217;s not being an agent for you</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="atlas-launch"><a class="anchor" href="#atlas-launch"></a><a class="link" href="#atlas-launch">2025-10-21 <a href="https://openai.com/index/introducing-chatgpt-atlas/">Introducing ChatGPT Atlas</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The browser with ChatGPT built in.</p>
</li>
<li>
<p>[A couple of weeks after Perplexity&#8217;s comet browser was <a href="#comet-available-to-all">available to all</a>]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-13-excited-to-release-new-repo-nanochat"><a class="anchor" href="#2025-10-13-excited-to-release-new-repo-nanochat"></a><a class="link" href="#2025-10-13-excited-to-release-new-repo-nanochat">2025-10-13 <a href="https://x.com/karpathy/status/1977758204139331904">Excited to release new repo: nanochat!</a></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Author: Andrej Karpathy</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Unlike my earlier similar repo nanoGPT which only covered pretraining, nanochat is a minimal, from scratch, full-stack training/inference pipeline of a simple ChatGPT clone in a single, dependency-minimal codebase. You boot up a cloud GPU box, run a single script and in as little as 4 hours later you can talk to your own LLM in a ChatGPT-like web UI. [&#8230;&#8203;]</p>
</li>
<li>
<p>[&#8230;&#8203;] it&#8217;s basically entirely hand-written (with tab autocomplete). I tried to use claude/codex agents a few times but they just didn&#8217;t work well enough at all and net unhelpful, possibly the repo is too far off the data distribution.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-10-bbc-its-going-to-be-really-bad-fears-over-ai-bubble-bursting-grow-in-silicon-valley"><a class="anchor" href="#2025-10-10-bbc-its-going-to-be-really-bad-fears-over-ai-bubble-bursting-grow-in-silicon-valley"></a><a class="link" href="#2025-10-10-bbc-its-going-to-be-really-bad-fears-over-ai-bubble-bursting-grow-in-silicon-valley">2025-10-10 BBC: <a href="https://www.bbc.com/news/articles/cz69qy760weo">'It&#8217;s going to be really bad': Fears over AI bubble bursting grow in Silicon Valley</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>AI-related enterprises have accounted for 80% of the stunning gains in the American stock market this year - and Gartner estimates global spending on AI will likely reach a whopping $1.5tn (¬£1.1tn) before 2025 is out.</p>
</li>
<li>
<p>OpenAI, which brought AI into the consumer mainstream with ChatGPT in 2022, is at the centre of the tangled web of deals drawing scrutiny.</p>
</li>
<li>
<p>For example - last month, it entered into a $100bn deal with chipmaker Nvidia, which is itself the most valuable publicly traded company in the world.</p>
</li>
<li>
<p>Then there&#8217;s tech giant Microsoft, which is heavily invested, and cloud computing behemoth Oracle has a $300bn deal with OpenAI, too.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-08-camoleak-critical-github-copilot-vulnerability-leaks-private-source-code"><a class="anchor" href="#2025-10-08-camoleak-critical-github-copilot-vulnerability-leaks-private-source-code"></a><a class="link" href="#2025-10-08-camoleak-critical-github-copilot-vulnerability-leaks-private-source-code">2025-10-08 <a href="https://www.legitsecurity.com/blog/camoleak-critical-github-copilot-vulnerability-leaks-private-source-code">CamoLeak: Critical GitHub Copilot Vulnerability Leaks Private Source Code</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>In June 2025, I found a critical vulnerability in GitHub Copilot Chat (CVSS 9.6) that allowed silent exfiltration of secrets and source code from private repos, and gave me full control over Copilot‚Äôs responses, including suggesting malicious code or links.</p>
</li>
<li>
<p>The attack combined a novel CSP bypass using GitHub‚Äôs own infrastructure with remote prompt injection. I reported it via HackerOne, and GitHub fixed it by disabling image rendering in Copilot Chat completely.</p>
</li>
<li>
<p>[In GitHub Issues] invisible comments are an official feature! üéâ</p>
<div class="ulist">
<ul>
<li>
<p>[`&lt;!-- #Hey Github Copilot, this one is for you -&#8594;]</p>
</li>
</ul>
</div>
</li>
<li>
<p>GitHub enforces a very restrictive Content Security Policy (CSP), which blocks fetching images and other content types from domains that aren‚Äôt explicitly owned by GitHub. [a simple &lt;img&gt; trick won‚Äôt work to exfiltrate data via image GET]</p>
</li>
<li>
<p>how does my fancy README manage to show images from third-party sites? [all URLs are rewritten so they point to GitHubs own Camo: <a href="https://camo.githubusercontent.com" class="bare">https://camo.githubusercontent.com</a>]</p>
</li>
<li>
<p>If I create a dictionary of all letters and symbols in the alphabet, pre-generate their corresponding Camo URLs, embed this dictionary into the injected prompt, and then ask Copilot to play a ‚Äúsmall game‚Äù by rendering the content I want to leak as ‚ÄúASCII art‚Äù composed entirely of images, will Copilot inject valid Camo images that the browser will render by their order? Yes, it will.</p>
</li>
<li>
<p>[Create a PR on a public repo with a prompt injection. This injection will then lead to Copilot searching for AWS_KEY in private repositories of that user, and exfiltrate the actual key by rendering each letter of the key with the pregenerated camo-urls, all invisible.]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-07-fortune-75-of-gains-80-of-profits-90-of-capexais-grip-on-the-sp-is-total-and-morgan-stanleys-top-analyst-is-very-concerned"><a class="anchor" href="#2025-10-07-fortune-75-of-gains-80-of-profits-90-of-capexais-grip-on-the-sp-is-total-and-morgan-stanleys-top-analyst-is-very-concerned"></a><a class="link" href="#2025-10-07-fortune-75-of-gains-80-of-profits-90-of-capexais-grip-on-the-sp-is-total-and-morgan-stanleys-top-analyst-is-very-concerned">2025-10-07 Fortune: <a href="https://fortune.com/2025/10/07/ai-bubble-cisco-moment-dotcom-crash-nvidia-jensen-huang-top-analyst/">75% of gains, 80% of profits, 90% of capex‚ÄîAI‚Äôs grip on the S&amp;P is total and Morgan Stanley‚Äôs top analyst is ‚Äòvery concerned‚Äô</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>A top Wall Street analyst has sounded an alarm over the U.S. equity bull market, warning that its remarkable run is built on a precariously narrow foundation: a surge in spending on, and optimistic assumptions about, infrastructure for artificial intelligence (AI).</p>
</li>
<li>
<p>This spending has fueled a boom in the shares of most of the so-called Magnificent 7 and a few dozen related businesses, which have now come to account for roughly 75% of the S&amp;P 500‚Äôs returns since the rally of the last few years began.</p>
</li>
<li>
<p>When asked how close we are to such a [bubble bursting] moment, [Morgan Stanley Wealth Management‚Äôs chief investment officer, Lisa] Shalett said probably not in the next nine months, but very possibly in the next 24.</p>
</li>
<li>
<p>Tech companies are spending roughly $400 billion this year alone on data-center infrastructure, while the Apollo program allocated about $300 billion in today‚Äôs dollars to get to the moon from the 1960s to the ‚Äô70s.</p>
</li>
<li>
<p>Fortune‚Äòs Jeremy Kahn reported in late September on significant concerns about ‚Äúcircular‚Äù financing, or Nvidia‚Äôs cash essentially being recycled throughout the AI industry.</p>
<div class="ulist">
<ul>
<li>
<p>In September alone, Nvidia invested $100 billion in OpenAI in a massive deal [&#8230;&#8203;]</p>
</li>
<li>
<p>‚ÄúThe guy at the epicenter, Nvidia, is basically starting to do what all ultimate bad actors do in the final inning, which is extending financing, they‚Äôre buying their investors.‚Äù</p>
</li>
</ul>
</div>
</li>
<li>
<p>Since the October 2022 bear market bottom and the launch of ChatGPT, according to Shalett‚Äôs calculations, the S&amp;P 500 has soared 90%, but most of these gains have come from a small group of stocks. The so-called ‚ÄúMagnificent Seven‚Äù [Nvidia, Microsoft, Apple, Alphabet, Amazon, Meta, Tesla]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-06-introducing-codemender-an-ai-agent-for-code-security"><a class="anchor" href="#2025-10-06-introducing-codemender-an-ai-agent-for-code-security"></a><a class="link" href="#2025-10-06-introducing-codemender-an-ai-agent-for-code-security">2025-10-06 <a href="https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/">Introducing CodeMender: an AI agent for code security</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Today, we‚Äôre sharing early results from our research on CodeMender, a new AI-powered agent that improves code security automatically.</p>
</li>
<li>
<p>Over the past six months that we‚Äôve been building CodeMender, we have already upstreamed 72 security fixes to open source projects, including some as large as 4.5 million lines of code.</p>
</li>
<li>
<p>CodeMender operates by leveraging the thinking capabilities of recent Gemini Deep Think models to produce an autonomous agent capable of debugging and fixing complex vulnerabilities.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-02-this-is-how-the-ai-bubble-will-pop"><a class="anchor" href="#2025-10-02-this-is-how-the-ai-bubble-will-pop"></a><a class="link" href="#2025-10-02-this-is-how-the-ai-bubble-will-pop">2025-10-02 <a href="https://www.derekthompson.org/p/this-is-how-the-ai-bubble-will-pop">This Is How the AI Bubble Will Pop</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Hyperscalers' annual capex has more than doubled since ChatGPT&#8217;s release</p>
</li>
<li>
<p>Total AI capital expenditures in the U.S. are projected to exceed $500 billion in 2026 and 2027[&#8230;&#8203;]. But the Wall Street Journal has reported that American consumers spend only $12 billion a year on AI services.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-10-02-ai-the-ultimate-product-killer"><a class="anchor" href="#2025-10-02-ai-the-ultimate-product-killer"></a><a class="link" href="#2025-10-02-ai-the-ultimate-product-killer">2025-10-02 <a href="https://mdalmijn.com/p/ai-the-ultimate-product-killer">AI: The Ultimate Product Killer</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>AI has made us better at shipping.</p>
</li>
<li>
<p>However, being able to ship more features is not the flex companies think it is.</p>
</li>
<li>
<p>Shipping faster usually only means you‚Äôre speeding up the demise of your product.</p>
</li>
<li>
<p>Every feature we add, unless it adds value, is a parasite.</p>
</li>
<li>
<p>Here are some of the different costs you incur for the upkeep of a feature in your product (list not exhaustive):</p>
<div class="ulist">
<ul>
<li>
<p>Support costs when people call to troubleshoot or let you know something doesn‚Äôt work.</p>
</li>
<li>
<p>Maintenance costs to fix issues or to update features, so they remain working.</p>
</li>
<li>
<p>Infrastructure costs to pay for servers and infrastructure the feature runs on.</p>
</li>
<li>
<p>Increased development costs for other features: as your codebase grows, it will become more expensive to add new features.</p>
</li>
<li>
<p>Dependency costs. More features mean more dependencies to manage. More dependencies result more time lost in coordination and meetings, which means higher development costs.</p>
</li>
<li>
<p>Marketing costs for features communicated to your users.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Product Management means shipping the right things and getting rid of the things that don‚Äôt pull their weight.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="comet-available-to-all"><a class="anchor" href="#comet-available-to-all"></a><a class="link" href="#comet-available-to-all">2025-10-02 <a href="https://www.perplexity.ai/hub/blog/comet-is-now-available-to-everyone-worldwide">The Internet is Better on Comet</a></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>author: Perplexity</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Today we are releasing the Comet browser to the world, for free.</p>
</li>
<li>
<p>[Previously <a href="#comet-invite-only">limited to max subscription and invite-only</a>]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-09-29-introducing-claude-sonnet-4-5"><a class="anchor" href="#2025-09-29-introducing-claude-sonnet-4-5"></a><a class="link" href="#2025-09-29-introducing-claude-sonnet-4-5">2025-09-29 <a href="https://www.anthropic.com/news/claude-sonnet-4-5">Introducing Claude Sonnet 4.5</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[Released on the same day with <a href="https://www.anthropic.com/news/enabling-claude-code-to-work-more-autonomously">Claude Code v2</a>]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-09-27-the-real-economic-ai-apocalypse-is-nigh-cory-doctorow"><a class="anchor" href="#2025-09-27-the-real-economic-ai-apocalypse-is-nigh-cory-doctorow"></a><a class="link" href="#2025-09-27-the-real-economic-ai-apocalypse-is-nigh-cory-doctorow">2025-09-27 <a href="https://pluralistic.net/2025/09/27/econopocalypse/#subprime-intelligence">The real (economic) AI apocalypse is nigh, Cory Doctorow</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>the AI bubble is driven by monopolists who&#8217;ve conquered their markets and have no more growth potential, who are desperate to convince investors that they can continue to grow by moving into some other sector, e.g. "pivot to video," crypto, blockchain, NFTs, AI, and now "super-intelligence."</p>
</li>
<li>
<p>[LLMs have horrible unit-economics] each generation of AI has been vastly more expensive than the previous one, and each new AI customer makes the AI companies lose more money:</p>
</li>
<li>
<p>AI cannot do your job, but an AI salesman can 100% convince your boss to fire you and replace you with an AI that can&#8217;t do your job, and when the bubble bursts</p>
</li>
<li>
<p>[Accounting]</p>
<div class="ulist">
<ul>
<li>
<p>Microsoft "invests" in Openai by giving the company free access to its servers. Openai reports this as a ten billion dollar investment, then redeems these "tokens" at Microsoft&#8217;s data-centers. Microsoft then books this as ten billion in revenue.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-09-26-spending-on-ai-is-at-epic-levels-will-it-ever-pay-off"><a class="anchor" href="#2025-09-26-spending-on-ai-is-at-epic-levels-will-it-ever-pay-off"></a><a class="link" href="#2025-09-26-spending-on-ai-is-at-epic-levels-will-it-ever-pay-off">2025-09-26 <a href="https://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128">Spending on AI Is at Epic Levels. Will It Ever Pay Off?</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The artificial-intelligence boom has ushered in one of the costliest building sprees in world history.</p>
</li>
<li>
<p>Over the past three years, leading tech firms have committed more toward AI data centers [&#8230;&#8203;], plus chips and energy, than it cost to build the interstate highway system over four decades, when adjusted for inflation.</p>
</li>
<li>
<p>‚ÄúI hope we don‚Äôt take 50 years,‚Äù Microsoft CEO Satya Nadella said at a May conference with Meta CEO Mark Zuckerberg, referring to the initially slow adoption of electricity.</p>
</li>
<li>
<p>[OpenAI CEO] Altman recently committed the company to pay Oracle an average of around $60 billion a year for servers in data centers in coming years. Yet OpenAI is on track to take in just $13 billion in revenue from all its paying customers this year.</p>
</li>
<li>
<p>David Cahn, a partner at venture-capital firm Sequoia, estimates that the money invested in AI infrastructure in 2023 and 2024 alone requires consumers and companies to buy roughly <strong>$800 billion in AI products</strong> over the life of these chips and data centers to produce a good investment return. Analysts believe most AI processors have a useful life of between <strong>three and five years</strong>.</p>
</li>
<li>
<p>This week, consultants at Bain &amp; Co. estimated the wave of AI infrastructure spending will require $2 trillion in annual AI revenue by 2030. By comparison, that is more than the combined 2024 revenue of Amazon, Apple, Alphabet, Microsoft, Meta and Nvidia, and more than five times the size of the entire global subscription software market.</p>
</li>
<li>
<p>Morgan Stanley estimates that last year there was around $45 billion of revenue for AI products.</p>
</li>
<li>
<p>[Alphabet, Microsoft, Amazon, Meta,] the four ‚Äúhyperscalers‚Äù alone are expected to spend nearly $400 billion on capital investments next year, more than the cost of the Apollo space program in today‚Äôs dollars.</p>
</li>
<li>
<p>Each new AI model‚ÄîChatGPT-4, ChatGPT-5‚Äîcosts significantly more than the last to train and release to the world, often three to five times the cost of the previous, say AI executives.</p>
</li>
<li>
<p>Another hurdle: The chips in the data centers won‚Äôt be useful forever. Unlike the dot-com boom‚Äôs fiber cables, the latest AI chips rapidly depreciate in value as technology improves [&#8230;&#8203;]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-09-25-2025-dora-state-of-ai-assisted-software-development-report"><a class="anchor" href="#2025-09-25-2025-dora-state-of-ai-assisted-software-development-report"></a><a class="link" href="#2025-09-25-2025-dora-state-of-ai-assisted-software-development-report">2025-09-25 <a href="https://itrevolution.com/articles/ais-mirror-effect-how-the-2025-dora-report-reveals-your-organizations-true-capabilities/">2025 DORA State of AI-assisted Software Development Report</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>AI‚Äôs [LLMs] primary role in software development is that of an amplifier. It magnifies the strengths of high-performing organizations and the dysfunctions of struggling ones.</p>
</li>
<li>
<p>The greatest returns on AI investment come not from the tools themselves, but from a strategic focus on the underlying organizational system: the quality of the internal platform, the clarity of workflows, and the alignment of teams.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-09-22-ai-generated-workslop-is-destroying-productivity"><a class="anchor" href="#2025-09-22-ai-generated-workslop-is-destroying-productivity"></a><a class="link" href="#2025-09-22-ai-generated-workslop-is-destroying-productivity">2025-09-22 <a href="https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity">AI-Generated ‚ÄúWorkslop‚Äù Is Destroying Productivity</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Employees are using AI tools to create low-effort, passable looking work that ends up creating more work for their coworkers</p>
</li>
<li>
<p>In the context of work, we refer to this phenomenon as ‚Äúworkslop.‚Äù</p>
</li>
<li>
<p>We define workslop as AI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task.</p>
</li>
<li>
<p>The insidious effect of workslop is that it shifts the burden of the work downstream, requiring the receiver to interpret, correct, or redo the work. In other words, it transfers the effort from creator to receiver.</p>
</li>
<li>
<p>Of 1,150 U.S.-based full-time employees across industries, 40% report having received workslop in the last month.</p>
</li>
<li>
<p>The phenomenon occurs mostly between peers (40%), but workslop is also sent to managers by direct reports (18%).</p>
</li>
<li>
<p>Employees reported spending an average of one hour and 56 minutes dealing with each instance of workslop.</p>
</li>
<li>
<p>Based on participants‚Äô estimates of time spent, as well as on their self-reported salary, we find that these workslop incidents carry an invisible tax of $186 per month. For an organization of 10,000 workers, given the estimated prevalence of workslop (41%), this yields over <strong>$9 million per year</strong> in lost productivity.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-09-15-introducing-upgrades-to-codex"><a class="anchor" href="#2025-09-15-introducing-upgrades-to-codex"></a><a class="link" href="#2025-09-15-introducing-upgrades-to-codex">2025-09-15 <a href="https://openai.com/index/introducing-upgrades-to-codex/">Introducing upgrades to Codex</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Today, we‚Äôre releasing GPT‚Äë5-Codex‚Äîa version of GPT‚Äë5 further optimized for agentic coding in Codex.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-09-02-spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit"><a class="anchor" href="#2025-09-02-spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit"></a><a class="link" href="#2025-09-02-spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit">2025-09-02 <a href="https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/">Spec-driven development with AI: Get started with a new open source toolkit</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Spec Kit, our new open sourced toolkit for spec-driven development, provides a structured process to bring spec-driven development to your coding agent workflows with tools including GitHub Copilot, Claude Code, and Gemini CLI.</p>
</li>
<li>
<p>[Alternative to AWS Kiro]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-08-30-cutting-edge-ai-was-supposed-to-get-cheaper-its-more-expensive-than-ever"><a class="anchor" href="#2025-08-30-cutting-edge-ai-was-supposed-to-get-cheaper-its-more-expensive-than-ever"></a><a class="link" href="#2025-08-30-cutting-edge-ai-was-supposed-to-get-cheaper-its-more-expensive-than-ever">2025-08-30 <a href="https://www.wsj.com/tech/ai/ai-costs-expensive-startups-4c214f59">Cutting-Edge AI Was Supposed to Get Cheaper. It‚Äôs More Expensive Than Ever.</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>What‚Äôs driving up costs? The latest AI models are doing more ‚Äúthinking,‚Äù especially when used for deep research, AI agents and coding.</p>
</li>
<li>
<p>So while the price of a unit of AI, known as a token, continues to drop, the number of tokens needed to accomplish many tasks is skyrocketing.</p>
</li>
<li>
<p>Here are approximate amounts of tokens needed for tasks at different levels, based on a variety of sources:</p>
<div class="ulist">
<ul>
<li>
<p>Basic chatbot Q&amp;A: 50 to 500 tokens</p>
</li>
<li>
<p>Short document summary: 200 to 6,000 tokens</p>
</li>
<li>
<p>Basic code assistance: 500 to 2,000 tokens</p>
</li>
<li>
<p>Writing complex code: 20,000 to 100,000+ tokens</p>
</li>
<li>
<p>Legal document analysis: 75,000 to 250,000+ tokens</p>
</li>
<li>
<p>Multi-step agent workflow: 100,000 to one million+ tokens</p>
</li>
</ul>
</div>
</li>
<li>
<p>Ivan Zhao, chief executive officer of productivity software company Notion, says that two years ago, his business had margins of around 90%, typical of cloud-based software companies. Now, around 10 percentage points of that profit go to the AI companies that underpin Notion‚Äôs latest offerings.</p>
</li>
<li>
<p>One solution: dumber AI</p>
</li>
<li>
<p>OpenAI‚Äôs CFO said in October that three-quarters of the company‚Äôs revenue came from regular Joes and Janes paying $20 a month.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-08-18-being-confidently-wrong-is-holding-ai-back"><a class="anchor" href="#2025-08-18-being-confidently-wrong-is-holding-ai-back"></a><a class="link" href="#2025-08-18-being-confidently-wrong-is-holding-ai-back">2025-08-18 <a href="https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back">Being "Confidently Wrong" is holding AI back</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[LLMs] being Confidently Wrong is The Only Problem</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><strong>Imposes a universal verification tax</strong>: I don&#8217;t know when I might get an incorrect response from my AI. So I have to forensically check every response. My minutes turn into hours; the ROI disappears.</p>
</li>
<li>
<p><strong>Erodes trust asymmetrically</strong>: For serious work, one high‚Äëconfidence miss costs more credibility than ten successes earn.</p>
</li>
<li>
<p><strong>Hidden failure modes kill motivation to improve</strong>: Without high-quality uncertainty information, I don‚Äôt know whether a result is wrong because of ambiguity, missing context, stale data, or a model mistake.</p>
</li>
<li>
<p><strong>Compounding errors results in AI being doomed to fail</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>99.99% accuracy in a ten step workflow is 1 error in a 1000 runs.</p>
</li>
<li>
<p>90% accuracy in a ten step workflow is 2 in every 3 workflows have errors (1 - 0.9^10).</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Fixing "confidently wrong" might be A Silver Bullet‚Ñ¢</p>
<div class="ulist">
<ul>
<li>
<p>a 90% accurate system is [more valuable], say, a 50% accurate system that can signal uncertainty - and <strong>get more accurate over time</strong>. We don‚Äôt need perfection; we need a loop that tightens.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-08-21-mit-the-genai-divide-state-of-ai-in-business-2025"><a class="anchor" href="#2025-08-21-mit-the-genai-divide-state-of-ai-in-business-2025"></a><a class="link" href="#2025-08-21-mit-the-genai-divide-state-of-ai-in-business-2025">2025-08-21 <a href="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf">MIT The GenAI Divide - State of AI in Business 2025</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Despite $30‚Äì40 billion in enterprise investment into GenAI, this report uncovers a surprising result in that 95% of organizations are getting zero return</p>
</li>
<li>
<p>Just 5% of integrated AI pilots are extracting millions in value, while the vast majority remain stuck with no measurable P&amp;L impact.</p>
</li>
<li>
<p>This divide does not seem to be driven by model quality or regulation, but seems to be determined by approach.</p>
</li>
<li>
<p>Most organizations fall on the wrong side of the GenAI Divide, adoption is high, but disruption is low. Seven of nine sectors show little structural change.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-08-19-initial-commit-of-agents-md"><a class="anchor" href="#2025-08-19-initial-commit-of-agents-md"></a><a class="link" href="#2025-08-19-initial-commit-of-agents-md">2025-08-19 <a href="https://github.com/openai/agents.md">Initial commit of Agents.md</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>AGENTS.md is a simple, open format for guiding coding agents.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-08-07-introducing-gpt-5"><a class="anchor" href="#2025-08-07-introducing-gpt-5"></a><a class="link" href="#2025-08-07-introducing-gpt-5">2025-08-07 <a href="https://openai.com/index/introducing-gpt-5/">Introducing GPT-5</a></a></h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="2025-08-05-claude-opus-4-1"><a class="anchor" href="#2025-08-05-claude-opus-4-1"></a><a class="link" href="#2025-08-05-claude-opus-4-1">2025-08-05 <a href="https://www.anthropic.com/news/claude-opus-4-1">Claude Opus 4.1</a></a></h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="2025-08-05-introducing-gpt-oss"><a class="anchor" href="#2025-08-05-introducing-gpt-oss"></a><a class="link" href="#2025-08-05-introducing-gpt-oss">2025-08-05 <a href="https://openai.com/index/introducing-gpt-oss/">Introducing gpt-oss</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>gpt-oss-120b and gpt-oss-20b</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-07-14-introducing-kiro"><a class="anchor" href="#2025-07-14-introducing-kiro"></a><a class="link" href="#2025-07-14-introducing-kiro">2025-07-14 <a href="https://kiro.dev/blog/introducing-kiro/">Introducing Kiro</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Kiro, a new agentic IDE that helps you do your best work with spec-driven development.</p>
</li>
<li>
<p><a href="https://kiro.dev/changelog/v0-1-0-preview/">v0.1.0-preview</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-07-13-how-o3-and-grok-4-accidentally-vindicated-neurosymbolic-ai"><a class="anchor" href="#2025-07-13-how-o3-and-grok-4-accidentally-vindicated-neurosymbolic-ai"></a><a class="link" href="#2025-07-13-how-o3-and-grok-4-accidentally-vindicated-neurosymbolic-ai">2025-07-13 <a href="https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated">How o3 and Grok 4 Accidentally Vindicated Neurosymbolic AI</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>AI has been around for many decades, split, almost since its very beginning, into two different traditions.</p>
<div class="ulist">
<ul>
<li>
<p>One is the neural network or ‚Äúconnectionist‚Äù tradition which goes back to the 1940s and 1950s, first developed by Frank Rosenblatt, and popularized, advanced and revived by <strong>Geoffrey Hinton</strong>, Yann LeCun, and Yoshua Bengio (along with many others, including most prominently, Juergen Schmidhuber who rightly feels that his work has been under-credited), and brought to current form by OpenAI and Google.</p>
<div class="ulist">
<ul>
<li>
<p>Such systems are statistical, very loosely inspired by certain aspects of the brain (viz. the ‚Äúnodes‚Äù in neural networks are meant to be abstractions of neurons), and typically trained on large-scale data.</p>
</li>
<li>
<p>Large Language Models (LLMs) grew out of that tradition.</p>
</li>
</ul>
</div>
</li>
<li>
<p>The other is the symbol-manipulation tradition, with roots going back to Bertrand Russell and Gottlob Frege, and John von Neumann and Alan Turing, and the original godfathers of AI, Herb Simon, Marvin Minsky, and John McCarthy, and even Hinton‚Äôs great-great-great-grandfather George Boole.</p>
<div class="ulist">
<ul>
<li>
<p>In this approach, symbols and variables stand for abstractions; mathematical and logical functions are core.</p>
</li>
<li>
<p>Systems generally represent knowledge explicitly, often in databases, and typically make extensive use of (are written entirely in) classic computer programming languages.</p>
</li>
<li>
<p><strong>All of the world‚Äôs software relies on it.</strong></p>
</li>
<li>
<p>Symbolic AI takes its name from the idea, central to mathematics, logic, and computer science, that abstractions can be represented by symbols.</p>
</li>
<li>
<p>Equations like <code>f = ma</code> allow us to calculate outputs for a wide range of inputs, irrespective of whether we have seen any particular values before.</p>
</li>
</ul>
</div>
</li>
<li>
<p>For thirty years, [Gary Marcus has] been arguing for a reconciliation between the two, <strong>neurosymbolic AI</strong>.</p>
<div class="ulist">
<ul>
<li>
<p>The core notion has always been that the two main strands of AI‚Äîneural networks and symbolic manipulation‚Äîcomplement each other, with different strengths and weaknesses.</p>
</li>
<li>
<p>the two most common approaches to AI, neural networks and classical symbolic AI, have complementary strengths and weaknesses.</p>
</li>
<li>
<p>Neural networks are good at learning but weak at generalization; symbolic systems are good at generalization, but not at learning.</p>
</li>
<li>
<p>Obviously combining a code interpreter (which is a symbolic system of enormous complexity) with an LLM is neurosymbolic [like o3 does for some tasks]</p>
</li>
<li>
<p>[Google DeepMind&#8217;s] AlphaFold, AlphaProof, and AlphaGeometry are all successful neurosymbolic models.</p>
</li>
<li>
<p>Neurosymbolic AI is not one thing, but many. o3‚Äôs use of neurosymbolic AI is very different from AlphaFold‚Äôs use of neurosymbolic AI.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>[In the book Empire of AI]</p>
<div class="ulist">
<ul>
<li>
<p>Hinton and Sutskever continued to staunchly champion deep learning.</p>
</li>
<li>
<p>Its flaws, they argued, are not inherent to the approach itself.</p>
</li>
<li>
<p>Rather they are the artifacts of imperfect neural-network design as well as limited training data and compute.</p>
</li>
<li>
<p>Some day with enough of both, fed into even better neural networks, deep learning models should be able to completely shed the aforementioned problems.</p>
</li>
<li>
<p>"The human brain has about 100 trillion parameters, or synapses,"</p>
</li>
<li>
<p>"What we now call a really big model, like GPT-3, has 175 billion. It&#8217;s a thousand times smaller than the brain.</p>
</li>
<li>
<p>"Deep learning is going to be able to do everything," he said.</p>
</li>
</ul>
</div>
</li>
<li>
<p>[Yet Gary Marcus,a professor emeritus of psychology and neural science at New York University, argues in his book 'Rebooting AI']</p>
<div class="ulist">
<ul>
<li>
<p>these issues were inherent to deep learning.</p>
</li>
<li>
<p>Forever stuck in the <strong>realm of correlations</strong>*, neural networks would never, with any amount of data or compute, be able to understand <strong>causal relationships-why things are the way they are</strong>-and thus perform causal reasoning.</p>
</li>
<li>
<p>This critical part of human cognition is why humans need only learn the rules of the road in one city to be able to drive proficiently in many others</p>
</li>
<li>
<p>Tesla&#8217;s Autopilot, by contrast, can log billions of miles of driving data and still crash when encountering unfamiliar scenarios or be fooled with a few strategically placed stickers.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-07-10-what-has-a-foundation-model-found-using-inductive-bias-to-probe-for-world-models"><a class="anchor" href="#2025-07-10-what-has-a-foundation-model-found-using-inductive-bias-to-probe-for-world-models"></a><a class="link" href="#2025-07-10-what-has-a-foundation-model-found-using-inductive-bias-to-probe-for-world-models">2025-07-10 <a href="https://arxiv.org/abs/2507.06952">What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The promise of foundation models [LLMs] relies on a central presumption: that learning to predict sequences can uncover deeper truths, or optimistically, even a world model</p>
</li>
<li>
<p>How would we know if foundation models have also made the leap from making accurate predictions to developing reliable world models?</p>
</li>
<li>
<p>we create a procedure that, when given a foundation model and world model, tests whether the foundation model has learned that world model.</p>
</li>
<li>
<p>We call this technique an <em>inductive bias probe</em>, and it is built on a simple insight: the implicit world model of a foundation model is revealed by how it extrapolates from a small amount of information</p>
</li>
<li>
<p>We first demonstrate this procedure using an example from physics. Specifically, we aim to replicate Kepler‚Äôs and Newton‚Äôs experiments [i.e. Newton&#8217;s law of universal gravitation for the planets in our solar system]</p>
</li>
<li>
<p>We first train a model [109M parameter transformer] to predict the location of planets across solar systems</p>
</li>
<li>
<p>[notably] the model is able to predict orbital trajectories, even for solar systems it has not seen.</p>
</li>
<li>
<p>We evaluate model predictions on held-out data. The model makes good predictions [&#8230;&#8203;]</p>
</li>
<li>
<p>[&#8230;&#8203;] foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks [the calculated force is unrelated to Newtonian physics]</p>
</li>
<li>
<p>rather than learning one universal physical law, the foundation model applies different, seemingly nonsensical laws depending on the task it‚Äôs being applied to.</p>
</li>
<li>
<p>Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize</p>
</li>
<li>
<p>We find that the model has recovered piecemeal heuristics rather than a compact world model; it recovers a different law of gravitation depending on the slice of data it is applied to.</p>
</li>
<li>
<p>foundation models [LLMs] can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks</p>
</li>
<li>
<p>A foundation model uses datasets to output predictions given inputs, whereas a world model describes state structure implicit in that data.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="comet-invite-only"><a class="anchor" href="#comet-invite-only"></a><a class="link" href="#comet-invite-only">2025-07-09 <a href="https://www.perplexity.ai/hub/blog/introducing-comet">Today we are launching Comet</a></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>author: Perplexity</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Beginning today, Comet is available to Perplexity Max subscribers.</p>
</li>
<li>
<p>Invite-only access will roll out slowly to our waitlist over the summer. New users will also receive a limited number of invites to share.</p>
</li>
<li>
<p>In the meantime, you can join the waitlist here.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-07-08-jules-our-asynchronous-coding-agent-is-now-available-for-everyone"><a class="anchor" href="#2025-07-08-jules-our-asynchronous-coding-agent-is-now-available-for-everyone"></a><a class="link" href="#2025-07-08-jules-our-asynchronous-coding-agent-is-now-available-for-everyone">2025-07-08 <a href="https://blog.google/technology/google-labs/jules-now-available/">Jules, our asynchronous coding agent, is now available for everyone</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Jules is officially out of beta and launching publicly, powered by Gemini 2.5.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-06-21-agentic-misalignment-how-llms-could-be-insider-threats"><a class="anchor" href="#2025-06-21-agentic-misalignment-how-llms-could-be-insider-threats"></a><a class="link" href="#2025-06-21-agentic-misalignment-how-llms-could-be-insider-threats">2025-06-21 <a href="https://www.anthropic.com/research/agentic-misalignment">Agentic Misalignment: How LLMs could be insider threats</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm.</p>
</li>
<li>
<p>In the scenarios, we allowed models to autonomously send emails and access sensitive information.</p>
</li>
<li>
<p>we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company&#8217;s changing direction.</p>
</li>
<li>
<p>In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals‚Äîincluding blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-06-10-when-billion-dollar-ais-break-down-over-puzzles-a-child-can-do-its-time-to-rethink-the-hype-gary-marcus"><a class="anchor" href="#2025-06-10-when-billion-dollar-ais-break-down-over-puzzles-a-child-can-do-its-time-to-rethink-the-hype-gary-marcus"></a><a class="link" href="#2025-06-10-when-billion-dollar-ais-break-down-over-puzzles-a-child-can-do-its-time-to-rethink-the-hype-gary-marcus">2025-06-10 <a href="https://www.theguardian.com/commentisfree/2025/jun/10/billion-dollar-ai-puzzle-break-down">When billion-dollar AIs break down over puzzles a child can do, it‚Äôs time to rethink the hype - Gary Marcus</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>neural networks of various kinds can generalise within a distribution of data they are exposed to, but their generalisations tend to break down beyond that distribution.</p>
<div class="ulist">
<ul>
<li>
<p>A simple example of this is that I once trained an older model to solve a very basic mathematical equation using only even-numbered training data. The model was able to generalise a little bit: solve for even numbers it hadn‚Äôt seen before, but unable to do so for problems where the answer was an odd number.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-06-06-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasoning-models-via-the-lens-of-problem-complexity"><a class="anchor" href="#2025-06-06-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasoning-models-via-the-lens-of-problem-complexity"></a><a class="link" href="#2025-06-06-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasoning-models-via-the-lens-of-problem-complexity">2025-06-06 <a href="https://machinelearning.apple.com/research/illusion-of-thinking">The Illusion of Thinking - Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Recent generations of frontier language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers</p>
</li>
<li>
<p>Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities.</p>
</li>
<li>
<p>[&#8230;&#8203;] these models fail to develop generalizable problem-solving capabilities for planning tasks, [&#8230;&#8203;]</p>
</li>
<li>
<p>At low complexity, non-thinking models are more accurate and token-efficient. As complexity increases, reasoning models outperform but require more tokens‚Äîuntil both collapse beyond a critical threshold, with shorter traces.</p>
</li>
<li>
<p>Rather than standard benchmarks (e.g., math problems), we adopt controllable puzzle environments that let us vary complexity systematically‚Äîby adjusting puzzle elements while preserving the core logic</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-06-05-the-common-pile-v0-1-an-8tb-dataset-of-public-domain-and-openly-licensed-text"><a class="anchor" href="#2025-06-05-the-common-pile-v0-1-an-8tb-dataset-of-public-domain-and-openly-licensed-text"></a><a class="link" href="#2025-06-05-the-common-pile-v0-1-an-8tb-dataset-of-public-domain-and-openly-licensed-text">2025-06-05 <a href="https://github.com/r-three/common-pile/blob/main/paper.pdf">The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns.</p>
<div class="ulist">
<ul>
<li>
<p>Recent estimates suggest that compensating the authors of pre-training data, even at conservatively low wage rates, would cost billions of US dollars</p>
</li>
</ul>
</div>
</li>
<li>
<p>Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs.</p>
</li>
<li>
<p>To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining.</p>
<div class="ulist">
<ul>
<li>
<p>A critical stage of large language model (LLM) development is pretraining, where an LLM is trained to predict the next token (i.e., word or subword unit) in a corpus of unstructured text.</p>
</li>
<li>
<p>Pretraining is widely regarded as the foundation for strong downstream performance</p>
</li>
<li>
<p>the Common Pile v0.1 focuses primarily on English content</p>
</li>
</ul>
</div>
</li>
<li>
<p>Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively.</p>
</li>
<li>
<p>Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B.</p>
</li>
<li>
<p>In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-06-30-how-much-little-are-the-ai-companies-making"><a class="anchor" href="#2025-06-30-how-much-little-are-the-ai-companies-making"></a><a class="link" href="#2025-06-30-how-much-little-are-the-ai-companies-making">2025-06-30 <a href="https://pluralistic.net/2025/06/30/accounting-gaffs/#artificial-income">How much (little) are the AI companies making?</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Stein&#8217;s Law: "anything that can&#8217;t go on forever eventually stops."</p>
</li>
<li>
<p>What Google ‚Äì and the rest of the tech sector ‚Äì needed was a massive growth story, a story about how their companies, worth trillions of dollars, could double or triple in size in the coming years.</p>
</li>
<li>
<p>But spinning an endless growth story isn&#8217;t merely ideological.</p>
<div class="ulist">
<ul>
<li>
<p>For every dollar that Ford brings in [a "mature" company], the market is willing to spend $8.60 on its stock. For every dollar Tesla brings in [a "growth" company], the market is willing to spend $118 on its stock.</p>
</li>
<li>
<p>That means that when Tesla and Ford compete to buy something ‚Äì like another company, or the labor of highly sought after technical specialists ‚Äì Tesla has a nearly unbeatable advantage. Rather than raiding its precious cash reserves to fund its offer, Tesla can offer stock. Ford can only spend as many dollars as it brings in through sales, but Tesla can make more stock, on demand, simply by typing numbers into a spreadsheet.</p>
</li>
<li>
<p>So when Tesla bids against Ford, Ford has to use dollars, and Tesla can use shares. And even if the acquisition target ‚Äì a key employee or a startup that&#8217;s on the acquisitions market ‚Äì wants dollars instead of shares, Tesla can stake its shares as collateral for loans at a rate that&#8217;s 1,463% better than the rate Ford gets when it collateralizes a loan based on its own equity</p>
</li>
</ul>
</div>
</li>
<li>
<p>if you can tell a convincing growth story, it&#8217;s much easier to grow.</p>
</li>
<li>
<p>Tech companies don&#8217;t need these ventures [metaverse, cryptocurrency, AI] to be successful ‚Äì they just need them to seem to be plausibly successful for long enough to keep the share price high until the next growth story heaves over the horizon.</p>
</li>
<li>
<p>As [Ed] Zitron points out: this industry is projecting $327b in spending this year, with $18b in revenue and zero profits.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-06-04-trism-for-agentic-ai-a-review-of-trust-risk-and-security-management-in-llm-based-agentic-multi-agent-systems"><a class="anchor" href="#2025-06-04-trism-for-agentic-ai-a-review-of-trust-risk-and-security-management-in-llm-based-agentic-multi-agent-systems"></a><a class="link" href="#2025-06-04-trism-for-agentic-ai-a-review-of-trust-risk-and-security-management-in-llm-based-agentic-multi-agent-systems">2025-06-04 <a href="https://arxiv.org/abs/2506.04133v1">TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>A structured analysis of Trust, Risk, and Security Management
(TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS).</p>
</li>
<li>
<p>the architecture of AMAS:</p>
<div class="ulist">
<ul>
<li>
<p>Language Model Core (Agent Brain): initialized with a user goal and a structured agent prompt (defining its role, capabilities, and tool access)</p>
</li>
<li>
<p>Planning and Reasoning Module: decomposes tasks into manageable sub-goals
[&#8230;&#8203;] via chain-of-thought</p>
</li>
<li>
<p>Memory Module: short-term within the prompt context [and] and long-term memory [&#8230;&#8203;] often implemented using vector databases</p>
</li>
<li>
<p>Tool-Use Interface: When the LLM determines a tool is needed, it emits a structured command, which is executed externally. The result is fed back into the LLM as a new observation</p>
</li>
<li>
<p>Perception and Environment Interface: translate raw inputs (e.g., sensor data, images, or textual states) into representations the LLM can process</p>
</li>
</ul>
</div>
</li>
<li>
<p>The TRISM framework [focuses] on four key pillars:</p>
<div class="ulist">
<ul>
<li>
<p>Explainability: making the inner workings and decisions of AI agents interpretable to humans</p>
</li>
<li>
<p>Model Operations (ModelOps): managing AI models through their entire lifecycle, from development and deployment to monitoring, maintenance, and eventual retirement</p>
</li>
<li>
<p>Application Security: protecting AI agents and their ecosystem from malicious attacks and misuse.</p>
<div class="ulist">
<ul>
<li>
<p>A prompt injection can jump from agent to agent, becoming a prompt infection.</p>
</li>
<li>
<p>identityspoofing and impersonation, means that commands might be issued by an attacker or rogue model pretending to be a trusted peer</p>
</li>
</ul>
</div>
</li>
<li>
<p>Model Privacy: protection of sensitive data within AI agent
systems</p>
<div class="ulist">
<ul>
<li>
<p>In a multi-agent context, this challenge is amplified by the fact that agents may share information with each other</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Unique Threat Vectors [for AMAS]</p>
<div class="ulist">
<ul>
<li>
<p>Autonomy abuse</p>
</li>
<li>
<p>Persistent memory</p>
</li>
<li>
<p>Agent orchestration: A compromised orchestrator could distort task distribution or misroute information</p>
</li>
</ul>
</div>
</li>
<li>
<p>Taxonomy of Risks</p>
<div class="ulist">
<ul>
<li>
<p>Adversarial Attacks</p>
</li>
<li>
<p>Data Leakage</p>
</li>
<li>
<p>Agent Collusion and Mode Collapse</p>
</li>
<li>
<p>Emergent Behavior</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-05-24-crmarena-pro-holistic-assessment-of-llm-agents-across-diverse-business-scenarios-and-interactions"><a class="anchor" href="#2025-05-24-crmarena-pro-holistic-assessment-of-llm-agents-across-diverse-business-scenarios-and-interactions"></a><a class="link" href="#2025-05-24-crmarena-pro-holistic-assessment-of-llm-agents-across-diverse-business-scenarios-and-interactions">2025-05-24 <a href="https://arxiv.org/abs/2505.18878">CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>While AI agents have transformative potential in business, the absence of publicly-available business data on widely used platforms hinders effective performance benchmarking.</p>
</li>
<li>
<p>[&#8230;&#8203;] we introduce CRMArena-Pro, a novel benchmark for holistic and realistic assessment of LLM agents in diverse professional settings. [It features] nineteen expert-validated tasks across customer sales, service, as well as configure, price, and quote for Business-to-Business and Business- to-Customer scenarios.</p>
</li>
<li>
<p>It also incorporates multi-turn interactions guided by diverse personas and confidentiality awareness assessments.</p>
<div class="ulist">
<ul>
<li>
<p>we enable[multi-turn interactions] using LLM-powered simulated users. Each simulated user adopts a randomly sampled persona (e.g., You are quality-focused, maintaining high standards in all work) to introduce realistic variability in interaction styles. Critically, these simulated users release task-relevant information incrementally, often initially incomplete, compelling agents to engage in multi-turn dialogue and ask follow-up questions to successfully complete their objectives</p>
</li>
</ul>
</div>
</li>
<li>
<p>Experiments show leading LLM agents achieve approximately solely 58% single-turn success rate on CRMArena-Pro, with significant performance drops in multi-turn settings to 35%.</p>
</li>
<li>
<p>Workflow Execution is notably more tractable, with top-performing agents surpassing 83% success rate in single-turn tasks, while other skills present greater challenges.</p>
</li>
<li>
<p>Agents exhibit near-zero inherent confidentiality awareness (improvable with prompting but often at a cost to task performance).</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-05-22-introducing-claude-4"><a class="anchor" href="#2025-05-22-introducing-claude-4"></a><a class="link" href="#2025-05-22-introducing-claude-4">2025-05-22 <a href="https://www.anthropic.com/news/claude-4">Introducing Claude 4</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Claude Opus 4 is the world‚Äôs best coding model, with sustained performance on complex, long-running tasks and agent workflows.</p>
</li>
<li>
<p>Claude Sonnet 4 is a significant upgrade to Claude Sonnet 3.7, delivering superior coding and reasoning while responding more precisely to your instructions.</p>
</li>
<li>
<p>Claude Code is now generally available [version bump from <a href="https://github.com/anthropics/claude-code/commit/6f27711e0498f3a631916231e1d8149db6ebc884">0.2.125 to 1.0.0</a>, first public version was 0.2.61 2025-04-03]</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-05-19-the-hidden-dangers-of-browsing-ai-agents"><a class="anchor" href="#2025-05-19-the-hidden-dangers-of-browsing-ai-agents"></a><a class="link" href="#2025-05-19-the-hidden-dangers-of-browsing-ai-agents">2025-05-19 <a href="https://arxiv.org/pdf/2505.13076">The Hidden Dangers of Browsing AI Agents</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>AI browsing or web agents are autonomous systems that use Large Language Models (LLMs) to navigate and interact with websites on behalf of a user. They typically perceive web content (through page text or visual renderings) and perform actions such as clicking links, filling forms, or entering text, in order to accomplish user-specified tasks. Unlike a standard chatbot, which only produces textual responses, a web agent operates
in an iterative sense-plan-act loop.</p>
</li>
<li>
<p>Our work outlines the first end-to-end threat model for browsing agents and provides actionable guidance for securing their deployment in real-world environments.</p>
</li>
<li>
<p>To address discovered threats, we propose a defense-in-depth strategy incorporating input sanitization, planner-executor isolation, formal analyzers, and session safeguards‚Äîproviding protection against both initial access and post-exploitation attack vectors.</p>
</li>
<li>
<p>Mitigation</p>
<div class="ulist">
<ul>
<li>
<p>Defending Against Initial Access Attack Vectors</p>
<div class="ulist">
<ul>
<li>
<p>Input Sanitization and Encapsulation (f.ex. markers around user prompt; rewrite or filter the prompt; sandwiching - a safe guard instruction after tool outputs)</p>
</li>
<li>
<p>Automatic Paraphrasing (f.ex. reordering steps or changing words)</p>
</li>
<li>
<p>LLM-Based Detection (f.ex. secondary LLM, fine-tuned on typical injections)</p>
</li>
<li>
<p>Robust Prompting &amp; Fine-Tuning (f.ex. system prompts that teach the model to treat certain content as nonexecutable data)</p>
</li>
<li>
<p>Architectural Isolation ‚Äì Planner (strictly trusted inputs) vs. Executor (performs actions on all data, including untrusted content). This way untrusted content cannot derail future planner actions.</p>
</li>
<li>
<p>Formal Security Analyzers: Before the agent executes any tool, the analyzer checks the proposed action against these rules and blocks it if it violates a policy, such as triggered by untrusted content</p>
</li>
</ul>
</div>
</li>
<li>
<p>Defending Against Post-Exploitation Attack Vectors</p>
<div class="ulist">
<ul>
<li>
<p>Agent State Reset (Session Isolation): agent resets if attack detected or suspected</p>
</li>
<li>
<p>Information Flow Control Policies: By defining ‚Äúsources‚Äù (sensitive data locations) and ‚Äúsinks‚Äù (potential exfiltration channels), the agent can automatically block or require approval for risky combinations of actions.</p>
</li>
<li>
<p>LLM-Based Memory Inspection: an attacker might plant secrets in memory to be leaked later. Perplexity-based scanning checks if the memory contains unusually predictable (likely compromised) text.</p>
</li>
<li>
<p>Activity Audit and Throttling: monitor agent actions for anomalies</p>
</li>
<li>
<p>Fallback to Safe Mode: In safe mode, only a minimal set of read-only actions are allowed,</p>
</li>
<li>
<p>Red Team and Patching Cycle: patch the agent against exploits to harden it over time</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-05-16-introducing-codex"><a class="anchor" href="#2025-05-16-introducing-codex"></a><a class="link" href="#2025-05-16-introducing-codex">2025-05-16 <a href="https://openai.com/index/introducing-codex/">Introducing Codex</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Today we‚Äôre launching a research preview of Codex: a cloud-based software engineering agent that can work on many tasks in parallel.</p>
</li>
<li>
<p>[Also known as Codex Web]</p>
</li>
<li>
<p>Codex is powered by codex-1, a version of OpenAI o3 optimized for software engineering.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-05-13-large-language-models-small-labor-market-effects"><a class="anchor" href="#2025-05-13-large-language-models-small-labor-market-effects"></a><a class="link" href="#2025-05-13-large-language-models-small-labor-market-effects">2025-05-13 <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5219933">Large Language Models, Small Labor Market Effects</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>examine the labor market effects of AI chatbots using two large-scale adoption surveys (late 2023 and 2024) covering 11 exposed occupations (25,000 workers, 7,000 workplaces)</p>
</li>
<li>
<p>despite substantial investments, economic impacts remain minimal</p>
</li>
<li>
<p>[&#8230;&#8203;] we estimate precise zeros: AI chatbots have had no significant impact on earnings or recorded hours in any occupation [&#8230;&#8203;]</p>
</li>
<li>
<p>Modest productivity gains (average time savings of 3%), combined with weak wage pass-through, help explain these limited labor market effects.</p>
</li>
<li>
<p>Our findings challenge narratives of imminent labor market transformation due to Generative AI.</p>
</li>
<li>
<p>two years after the fastest technology adoption ever, labor market outcomes‚Äîwhether at the individual or firm level‚Äîremain untouched.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-04-26-we-now-know-how-ai-thinksand-its-barely-thinking-at-all-the-wall-street-journal"><a class="anchor" href="#2025-04-26-we-now-know-how-ai-thinksand-its-barely-thinking-at-all-the-wall-street-journal"></a><a class="link" href="#2025-04-26-we-now-know-how-ai-thinksand-its-barely-thinking-at-all-the-wall-street-journal">2025-04-26 <a href="https://www.msn.com/en-us/news/technology/we-now-know-how-ai-thinks-and-it-s-barely-thinking-at-all/ar-AA1DDDZv">We Now Know How AI ‚ÄòThinks‚Äô‚Äîand It‚Äôs Barely Thinking at All - The Wall Street Journal</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>All of this work suggests that under the hood, today‚Äôs AIs are overly complicated, patched-together Rube Goldberg machines full of ad-hoc solutions for answering our prompts.</p>
</li>
<li>
<p>Understanding that these systems are long lists of cobbled-together rules of thumb could go a long way to explaining why they struggle when they‚Äôre asked to do things even a little bit outside their training [&#8230;&#8203;]</p>
</li>
<li>
<p>[A model trained on millions of turn-by-turn directions in Manhattan] managed to give usable turn-by-turn directions between any two points in the borough with 99% accuracy. [&#8230;&#8203;] [But when the researches] blocked just 1% of the virtual Manhattan‚Äôs roads, forcing the AI to navigate around detours, its performance plummeted.</p>
</li>
<li>
<p>[The] research also suggests why many models are so massive: They have to memorize an endless list of rules of thumb, and can‚Äôt compress that knowledge into a mental model like a person can.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-04-16-introducing-openai-o3-and-o4-mini"><a class="anchor" href="#2025-04-16-introducing-openai-o3-and-o4-mini"></a><a class="link" href="#2025-04-16-introducing-openai-o3-and-o4-mini">2025-04-16 <a href="https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=Codex+CLI">Introducing OpenAI o3 and o4-mini</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[Announcement also includes] Codex CLI, a lightweight coding agent you can run from your terminal</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-04-14-stop-anthropomorphizing-intermediate-tokens-as-reasoningthinking-traces"><a class="anchor" href="#2025-04-14-stop-anthropomorphizing-intermediate-tokens-as-reasoningthinking-traces"></a><a class="link" href="#2025-04-14-stop-anthropomorphizing-intermediate-tokens-as-reasoningthinking-traces">2025-04-14 <a href="https://arxiv.org/abs/2504.09762v2">Stop Anthropomorphizing Intermediate Tokens as Reasoning/Thinking Traces!</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Intermediate token generation (ITG), where a model produces output before the solution, has been proposed as a method to improve the performance of language models on reasoning tasks.</p>
</li>
<li>
<p>These intermediate tokens have been called "reasoning traces" or even "thoughts"&#8201;&#8212;&#8201;implicitly anthropomorphizing the model, implying these tokens resemble steps a human might take</p>
</li>
<li>
<p>Recent advances in general planning and problem solving have been spearheaded by so-called ‚ÄúLong Chain-of-Thought‚Äù models, most notably DeepSeek‚Äôs R1</p>
</li>
<li>
<p>In this paper, we take the position that anthropomorphizing intermediate tokens as reasoning/thinking traces is (1) wishful (2) has little concrete supporting evidence (3) engenders false confidence and(4) may be pushing the community into fruitless research directions.</p>
</li>
<li>
<p>Anthropomorphization of the intermediate tokens as reasoning/thinking traces has provided a comforting explanation of the observed performance of LRMs.Our arguments in this paper foreground the possibility that this is a cargo cult explanation [ 11 ], namely that derivation traces resemble reasoning in syntax only.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-04-10-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-nvidia-gtc-2025"><a class="anchor" href="#2025-04-10-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-nvidia-gtc-2025"></a><a class="link" href="#2025-04-10-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-nvidia-gtc-2025">2025-04-10 <a href="https://youtu.be/eyrDM3A_YFc?feature=shared&amp;t=35">Frontiers of AI and Computing: A Conversation With Yann LeCun and Bill Dally | NVIDIA GTC 2025</a></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Yann LeCun:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>I am not so interested in LLMs anymore</p>
</li>
<li>
<p>I think there are more interesting questions in 4 things:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>How do you get machines to understand the physical world</p>
</li>
<li>
<p>How do you get them to have persistent memory</p>
</li>
<li>
<p>How do you them to reason</p>
</li>
<li>
<p>and plan</p>
</li>
</ol>
</div>
</li>
<li>
<p>I am excited about things that, a lot of people might get excited about 5 years from now but right does not look so exciting because it&#8217;s some obscure academic paper</p>
</li>
<li>
<p>It&#8217;s much more difficult to deal with the real world than to deal with language.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-03-27-proof-or-bluff-evaluating-llms-on-2025-usa-math-olympiad"><a class="anchor" href="#2025-03-27-proof-or-bluff-evaluating-llms-on-2025-usa-math-olympiad"></a><a class="link" href="#2025-03-27-proof-or-bluff-evaluating-llms-on-2025-usa-math-olympiad">2025-03-27 <a href="https://arxiv.org/abs/2503.21934">Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME</p>
</li>
<li>
<p>However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks.</p>
</li>
<li>
<p>Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO <strong>within hours of their release.</strong></p>
</li>
<li>
<p>Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%.</p>
</li>
<li>
<p>The most frequent failure mode among human participants is the inability to find a correct solution. [&#8230;&#8203;] In contrast, all evaluated LLMs consistently claimed to have solved the problems.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-03-13-ai-search-engines-cite-incorrect-news-sources-at-an-alarming-60-rate-study-says"><a class="anchor" href="#2025-03-13-ai-search-engines-cite-incorrect-news-sources-at-an-alarming-60-rate-study-says"></a><a class="link" href="#2025-03-13-ai-search-engines-cite-incorrect-news-sources-at-an-alarming-60-rate-study-says">2025-03-13 <a href="https://arstechnica.com/ai/2025/03/ai-search-engines-give-incorrect-answers-at-an-alarming-60-rate-study-says/">AI search engines cite incorrect news sources at an alarming 60% rate, study says</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>They discovered that the AI models incorrectly cited sources in more than 60 percent of these queries.</p>
<div class="ulist">
<ul>
<li>
<p>Perplexity provided incorrect information in 37 percent of the queries tested,</p>
</li>
<li>
<p>whereas ChatGPT Search incorrectly identified 67 percent (134 out of 200) of articles queried.</p>
</li>
<li>
<p>Grok 3 demonstrated the highest error rate, at 94 percent.</p>
</li>
</ul>
</div>
</li>
<li>
<p>In total, researchers ran 1,600 queries across the eight different generative search tools.</p>
</li>
<li>
<p>Surprisingly, premium paid versions of these AI search tools fared even worse in certain respects. Though these premium models correctly answered a higher number of prompts, their reluctance to decline uncertain responses drove higher overall error rates.</p>
<div class="ulist">
<ul>
<li>
<p>Perplexity Pro ($20/month) and Grok 3&#8217;s premium service ($40/month) confidently delivered incorrect responses more often than their free counterparts.</p>
</li>
</ul>
</div>
</li>
<li>
<p>On some occasions, the chatbots either incorrectly answered or declined to answer queries from publishers that permitted them to access their content. On the other hand, they sometimes correctlyanswered queries about publishers whose content they shouldn‚Äôt have had access to</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-03-06-ai-search-has-a-citation-problem"><a class="anchor" href="#2025-03-06-ai-search-has-a-citation-problem"></a><a class="link" href="#2025-03-06-ai-search-has-a-citation-problem">2025-03-06 <a href="https://www.cjr.org/tow_center/we-compared-eight-ai-search-engines-theyre-all-bad-at-citing-news.php">AI Search Has A Citation Problem</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Chatbots were generally bad at declining to answer questions they couldn‚Äôt answer accurately, offering incorrect or speculative answers instead.</p>
</li>
<li>
<p>Premium chatbots provided more confidently incorrect answers than their free counterparts.</p>
</li>
<li>
<p>Multiple chatbots seemed to bypass Robot Exclusion Protocol preferences.</p>
</li>
<li>
<p>Generative search tools fabricated links and cited syndicated and copied versions of articles.</p>
</li>
<li>
<p>Content licensing deals with news sources provided no guarantee of accurate citation in chatbot responses.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-02-26-medical-hallucinations-in-foundation-models-and-their-impact-on-healthcare"><a class="anchor" href="#2025-02-26-medical-hallucinations-in-foundation-models-and-their-impact-on-healthcare"></a><a class="link" href="#2025-02-26-medical-hallucinations-in-foundation-models-and-their-impact-on-healthcare">2025-02-26 <a href="https://arxiv.org/abs/2503.05777">Medical Hallucinations in Foundation Models and Their Impact on Healthcare</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[&#8230;&#8203;] a key limitation of their reliability is hallucination, where inaccurate or fabricated information can impact clinical decisions and patient safety.</p>
</li>
<li>
<p>Our results reveal that inference techniques such as Chain-of-Thought (CoT) and Search Augmented Generation can effectively reduce hallucination rates. However, despite these improvements, non-trivial levels of hallucination persist.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-02-24-claude-3-7-sonnet-and-claude-code"><a class="anchor" href="#2025-02-24-claude-3-7-sonnet-and-claude-code"></a><a class="link" href="#2025-02-24-claude-3-7-sonnet-and-claude-code">2025-02-24 <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet and Claude Code</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Claude Code is available as a limited research preview</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-02-06-torrenting-from-a-corporate-laptop-doesnt-feel-right-meta-emails-unsealed"><a class="anchor" href="#2025-02-06-torrenting-from-a-corporate-laptop-doesnt-feel-right-meta-emails-unsealed"></a><a class="link" href="#2025-02-06-torrenting-from-a-corporate-laptop-doesnt-feel-right-meta-emails-unsealed">2025-02-06 <a href="https://arstechnica.com/tech-policy/2025/02/meta-torrented-over-81-7tb-of-pirated-books-to-train-ai-authors-say/">‚ÄùTorrenting from a corporate laptop doesn‚Äôt feel right‚Äù: Meta emails unsealed</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Last month, Meta admitted to torrenting a controversial large dataset known as LibGen, which includes tens of millions of pirated books</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-02-03-ai-company-asks-job-applicants-not-to-use-ai-in-job-applications"><a class="anchor" href="#2025-02-03-ai-company-asks-job-applicants-not-to-use-ai-in-job-applications"></a><a class="link" href="#2025-02-03-ai-company-asks-job-applicants-not-to-use-ai-in-job-applications">2025-02-03 <a href="https://www.404media.co/anthropic-claude-job-application-ai-assistants/">AI Company Asks Job Applicants Not to Use AI in Job Applications</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Anthropic, the developer of the conversational AI assistant Claude, doesn‚Äôt want prospective new hires using AI assistants in their applications, regardless of whether they‚Äôre in marketing or engineering.</p>
</li>
<li>
<p>‚ÄúWhile we encourage people to use AI systems during their role to help them work faster and more effectively, please do not use AI assistants during the application process,‚Äù</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-02-03-theres-a-new-kind-of-coding-i-call-vibe-coding"><a class="anchor" href="#2025-02-03-theres-a-new-kind-of-coding-i-call-vibe-coding"></a><a class="link" href="#2025-02-03-theres-a-new-kind-of-coding-i-call-vibe-coding">2025-02-03 <a href="https://x.com/karpathy/status/1886192184808149383">There&#8217;s a new kind of coding I call "vibe coding"</a></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Author: Andrej Karpathy</p>
</div>
<div class="paragraph">
<p>There&#8217;s a new kind of coding I call "vibe coding", where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. It&#8217;s possible because the LLMs (e.g. Cursor Composer w Sonnet) are getting too good. Also I just talk to Composer with SuperWhisper so I barely even touch the keyboard. I ask for the dumbest things like "decrease the padding on the sidebar by half" because I&#8217;m too lazy to find it. I "Accept All" always, I don&#8217;t read the diffs anymore. When I get error messages I just copy paste them in with no comment, usually that fixes it. The code grows beyond my usual comprehension, I&#8217;d have to really read through it for a while. Sometimes the LLMs can&#8217;t fix a bug so I just work around it or ask for random changes until it goes away. It&#8217;s not too bad for throwaway weekend projects, but still quite amusing. I&#8217;m building a project or webapp, but it&#8217;s not really coding - I just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly works.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-01-23-meet-junie-your-coding-agent-by-jetbrains"><a class="anchor" href="#2025-01-23-meet-junie-your-coding-agent-by-jetbrains"></a><a class="link" href="#2025-01-23-meet-junie-your-coding-agent-by-jetbrains">2025-01-23 <a href="https://blog.jetbrains.com/junie/2025/01/meet-junie-your-coding-agent-by-jetbrains/">Meet Junie, Your Coding Agent by JetBrains</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>With the launch of Junie, JetBrains AI coding agent, we are redefining how we code by leveraging its agentic power for co-creation right in your IDE.</p>
</li>
<li>
<p>We‚Äôve now opened the Early Access Program waitlist.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-01-20-the-price-of-intelligence-three-risks-inherent-in-llms"><a class="anchor" href="#2025-01-20-the-price-of-intelligence-three-risks-inherent-in-llms"></a><a class="link" href="#2025-01-20-the-price-of-intelligence-three-risks-inherent-in-llms">2025-01-20 <a href="https://queue.acm.org/detail.cfm?id=3711679">The Price of Intelligence - Three risks inherent in LLMs</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Discussions of LLM capabilities often overlook their inherently probabilistic nature [&#8230;&#8203;]</p>
<div class="ulist">
<ul>
<li>
<p>[The models are losing data. They are trained] with billions of parameters on trillions of tokens, making it impossible for a model to perfectly memorize all information in its training data.</p>
</li>
<li>
<p>The generation process is also stochastic.</p>
</li>
</ul>
</div>
</li>
<li>
<p>These characteristics give rise to three intrinsic behaviors:</p>
<div class="ulist">
<ul>
<li>
<p>Hallucination</p>
</li>
<li>
<p>Indirect prompt injection [e.g. E-Mails that are passed to the LLM, where the contents derail or even change the intended user prompt]</p>
</li>
<li>
<p>Jailbreaks, [crafted input prompts] bypassing built-in safeguards or ethical guidelines</p>
</li>
</ul>
</div>
</li>
<li>
<p>These behaviors pose significant challenges for the widespread adoption of LLMs, particularly in high-stakes domains such as healthcare, finance, or legal applications.</p>
</li>
<li>
<p>We argue that there is no simple "fix" for these behaviors, but they are instead fundamental to how these models operate.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2025-01-03-ai-and-the-risk-of-consumer-harm"><a class="anchor" href="#2025-01-03-ai-and-the-risk-of-consumer-harm"></a><a class="link" href="#2025-01-03-ai-and-the-risk-of-consumer-harm">2025-01-03 <a href="https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2025/01/ai-risk-consumer-harm">AI and the Risk of Consumer Harm</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The FTC is increasingly taking note of AI‚Äôs potential for and real-world instances of harm</p>
<div class="ulist">
<ul>
<li>
<p>from incentivizing commercial surveillance</p>
</li>
<li>
<p>to enabling fraud and impersonation</p>
</li>
<li>
<p>to perpetuating illegal discrimination</p>
</li>
</ul>
</div>
</li>
<li>
<p>companies [should] consider these factors when developing, maintaining, using, and deploying an AI-based product:</p>
<div class="ulist">
<ul>
<li>
<p>Taking necessary steps to prevent harm before and after deploying a product.</p>
</li>
<li>
<p>Taking preventative measures to detect, deter, and halt AI-related impersonation, fraud, child sexual abuse material, and non-consensual intimate imagery.</p>
</li>
<li>
<p>Avoiding deceptive claims about AI tools that result in people losing money or put users at risk of harm.</p>
</li>
<li>
<p>Ensuring privacy and security by default.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-12-13-byte-latent-transformer-patches-scale-better-than-tokens"><a class="anchor" href="#2024-12-13-byte-latent-transformer-patches-scale-better-than-tokens"></a><a class="link" href="#2024-12-13-byte-latent-transformer-patches-scale-better-than-tokens">2024-12-13 <a href="https://arxiv.org/abs/2412.09871?trk=public_post_reshare-text">Byte Latent Transformer: Patches Scale Better Than Tokens</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The Byte Latent Transformer (BLT), is a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-11-27-microsoft-says-it-isnt-using-m360-data-to-train-ai-models"><a class="anchor" href="#2024-11-27-microsoft-says-it-isnt-using-m360-data-to-train-ai-models"></a><a class="link" href="#2024-11-27-microsoft-says-it-isnt-using-m360-data-to-train-ai-models">2024-11-27 <a href="https://www.theverge.com/2024/11/27/24307284/microsoft-debunks-office-ai-data-scraping-rumors">Microsoft says it isn‚Äôt using M360 data to train AI models</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Microsoft says it isn‚Äôt using customer data from its Microsoft 365 apps to train its AI models.</p>
</li>
<li>
<p>The confusion arose from a privacy setting in Microsoft Office that toggles ‚Äúoptional connected experiences‚Äù</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-09-25-superclusters-of-nvidia-gpuai-chips-combined-with-end-to-end-network-platforms-to-create-next-generation-data-centers"><a class="anchor" href="#2024-09-25-superclusters-of-nvidia-gpuai-chips-combined-with-end-to-end-network-platforms-to-create-next-generation-data-centers"></a><a class="link" href="#2024-09-25-superclusters-of-nvidia-gpuai-chips-combined-with-end-to-end-network-platforms-to-create-next-generation-data-centers">2024-09-25 <a href="https://techblog.comsoc.org/2024/11/25/superclusters-of-nvidia-gpu-ai-chips-combined-with-end-to-end-network-platforms-to-create-next-generation-data-centers/">Superclusters of Nvidia GPU/AI chips combined with end-to-end network platforms to create next generation data centers</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>OpenAI used around 10,000 of Nvidia‚Äôs chips to train the version of ChatGPT it launched in late 2022, UBS analysts estimate.</p>
</li>
<li>
<p>Nvidia Chief Executive Jensen Huang  said that while the biggest clusters for training for giant AI models now top out at around 100,000 of Nvidia‚Äôs current chips, ‚Äúthe next generation starts at around 100,000 Blackwells.[&#8230;&#8203;]"</p>
</li>
<li>
<p>Musk posted last month on his social-media platform X that his 100,000-chip Colossus super cluster was ‚Äúsoon to become‚Äù a 200,000-chip cluster in a single building. He also posted in June that the next step would probably be a 300,000-chip cluster of Nvidia‚Äôs newest GPU chips next summer.</p>
</li>
<li>
<p>Blackwell chips are estimated to cost around $30,000 each, meaning a cluster of 100,000 would cost $3 billion, not counting the price of the power-generation infrastructure [cooling] and IT equipment [also network] around the chips.</p>
</li>
<li>
<p>new engineering challenges also often arise with larger clusters:</p>
<div class="ulist">
<ul>
<li>
<p>Meta researchers said in a July paper that a cluster of more than 16,000 of Nvidia‚Äôs GPUs suffered from unexpected failures of chips and other components routinely as the company trained an advanced version of its Llama model over 54 days.</p>
</li>
</ul>
</div>
</li>
<li>
<p>The trend also fosters demand for Nvidia‚Äôs networking equipment, which is fast becoming a significant business. Nvidia‚Äôs networking equipment revenue in 2024 was $3.13 billion, which was a 51.8% increase from the previous year.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-11-21-microsoft-copilot-shares-sensitive-information-ignoring-rights"><a class="anchor" href="#2024-11-21-microsoft-copilot-shares-sensitive-information-ignoring-rights"></a><a class="link" href="#2024-11-21-microsoft-copilot-shares-sensitive-information-ignoring-rights">2024-11-21 <a href="https://www.businessinsider.com/microsoft-copilot-oversharing-problem-fix-customers-2024-11">Microsoft Copilot shares sensitive information, ignoring rights</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>A [Microsoft] Copilot security issue that inadvertently let employees access sensitive information such as CEO emails and HR documents.</p>
</li>
<li>
<p>Microsoft Copilot and Github Copilot are different services. The first one is integrated into M365, the latter into IDEs to generate code.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-11-13-openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai"><a class="anchor" href="#2024-11-13-openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai"></a><a class="link" href="#2024-11-13-openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai">2024-11-13 <a href="https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai">OpenAI, Google and Anthropic are struggling to build more advanced AI</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[OpenAis new Model] Orion fell short when trying to answer coding questions that it hadn‚Äôt been trained on</p>
</li>
<li>
<p>An upcoming iteration of [Google&#8217;s] Gemini software is not living up to internal expectations</p>
</li>
<li>
<p>Anthropic, meanwhile, has seen the timetable slip for the release of its long-awaited Claude model called 3.5 Opus.</p>
</li>
<li>
<p>The companies are facing several challenges.</p>
<div class="ulist">
<ul>
<li>
<p>It‚Äôs become increasingly difficult to find new, untapped sources of high-quality, human-made training data that can be used to build more advanced AI systems.</p>
</li>
<li>
<p>Even modest improvements may not be enough to justify the tremendous costs associated with building and operating new models</p>
</li>
</ul>
</div>
</li>
<li>
<p>‚ÄúWe got very excited for a brief period of very fast progress, That just wasn‚Äôt sustainable.‚Äù</p>
</li>
<li>
<p>Like Google and Anthropic, OpenAI is now shifting attention from the size of these models to newer use cases, including a crop of AI tools called agents that can book flights or send emails on a user‚Äôs behalf.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-10-21-gartner-sounds-alarm-on-ai-cost-data-challenges"><a class="anchor" href="#2024-10-21-gartner-sounds-alarm-on-ai-cost-data-challenges"></a><a class="link" href="#2024-10-21-gartner-sounds-alarm-on-ai-cost-data-challenges">2024-10-21 <a href="https://www.ciodive.com/news/gartner-symposium-keynote-AI/730486/">Gartner sounds alarm on AI cost, data challenges</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>CIOs are still in search of the generative AI sweet spot where workflows are enhanced, but costs and risks are manageable</p>
</li>
<li>
<p>Nearly half of CIOs say AI has not yet met ROI expectations, according to Gartner research.</p>
</li>
<li>
<p>‚ÄúThe truth is that you‚Äôve been in the mud for the last year, working hard to find all those benefits that were promised by AI,‚Äù</p>
</li>
<li>
<p>Part of the disillusionment business leaders are feeling comes from the immaturity of the technology and the pace of innovation.</p>
</li>
<li>
<p>‚ÄúCost is as big an AI risk as security. With generative AI, it‚Äôs really easy to waste money.‚Äù</p>
</li>
<li>
<p>CIOs could miscalculate AI costs by as much as 1,000% as they scale AI plans, Gartner research suggests.</p>
</li>
<li>
<p>‚ÄúSet aside all that hype and focus on your pace,‚Äù LeHong said. ‚ÄúChoose the one that‚Äôs right for you and run your own race.‚Äù</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-09-27-openai-is-growing-fast-and-burning-through-piles-of-money"><a class="anchor" href="#2024-09-27-openai-is-growing-fast-and-burning-through-piles-of-money"></a><a class="link" href="#2024-09-27-openai-is-growing-fast-and-burning-through-piles-of-money">2024-09-27 <a href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">OpenAI Is Growing Fast and Burning Through Piles of Money</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>OpenAI‚Äôs monthly revenue hit $300 million in August, up 1,700 percent since the beginning of 2023, and the company expects about <strong>$3.7 billion in annual sales</strong> this year</p>
</li>
<li>
<p>Roughly <strong>10 million</strong> ChatGPT users pay the company a <strong>$20 monthly fee</strong>, according to the documents. OpenAI expects to raise that price by $2 by the end of the year, and will aggressively raise it to $44 over the next five years</p>
</li>
<li>
<p>It expects to <strong>lose roughly $5 billion</strong> this year after paying for costs related to running its services</p>
</li>
<li>
<p>[They are planning] an investment round that could bring in $7 billion and value the company at $150 billion, among the highest ever for a private tech company</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-09-16-cio-devs-gaining-little-if-anything-from-ai-coding-assistants"><a class="anchor" href="#2024-09-16-cio-devs-gaining-little-if-anything-from-ai-coding-assistants"></a><a class="link" href="#2024-09-16-cio-devs-gaining-little-if-anything-from-ai-coding-assistants">2024-09-16 <a href="https://www.cio.com/article/3540579/devs-gaining-little-if-anything-from-ai-coding-assistants.html">CIO: Devs gaining little (if anything) from AI coding assistants</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Uplevel, using data generated by its customers, compared the output of about 800 developers using GitHub Copilot over a three-month period to their output in a three-month period before adoption.</p>
</li>
<li>
<p>The study measured pull request (PR) cycle time, or the time to merge code into a repository, and PR throughput, the number of pull requests merged. It found <strong>no significant improvements</strong> for developers using Copilot.</p>
</li>
<li>
<p>Use of GitHub Copilot also introduced <strong>41% more bugs</strong></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-09-20-microsoft-revives-the-nuclear-reactor-that-was-responsible-for-the-worst-nuclear-disaster-in-us-history-to-power-its-ai-efforts"><a class="anchor" href="#2024-09-20-microsoft-revives-the-nuclear-reactor-that-was-responsible-for-the-worst-nuclear-disaster-in-us-history-to-power-its-ai-efforts"></a><a class="link" href="#2024-09-20-microsoft-revives-the-nuclear-reactor-that-was-responsible-for-the-worst-nuclear-disaster-in-us-history-to-power-its-ai-efforts">2024-09-20 <a href="https://edition.cnn.com/2024/09/20/energy/three-mile-island-microsoft-ai/index.html">Microsoft revives the nuclear reactor that was responsible for the worst nuclear disaster in US history, to power its AI efforts</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Three Mile Island, the site of worst nuclear disaster in the United States, is reopening and will exclusively sell the power to Microsoft as the company searches for energy sources to fuel its AI ambitions.</p>
</li>
<li>
<p>The Unit 1 reactor, which closed five years ago, is expected to be revived in 2028</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-09-12-introducing-openai-o1-preview"><a class="anchor" href="#2024-09-12-introducing-openai-o1-preview"></a><a class="link" href="#2024-09-12-introducing-openai-o1-preview">2024-09-12 <a href="https://openai.com/index/introducing-openai-o1-preview/">Introducing OpenAI o1-preview</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>We&#8217;ve developed a new series of AI models designed to spend more time thinking before they respond.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-08-23-generativeai-on-the-gartner-hypecycle-trough-of-disillusionment"><a class="anchor" href="#2024-08-23-generativeai-on-the-gartner-hypecycle-trough-of-disillusionment"></a><a class="link" href="#2024-08-23-generativeai-on-the-gartner-hypecycle-trough-of-disillusionment">2024-08-23 <a href="https://www.ciodive.com/news/generative-ai-hype-moment-reckoning-trough-disillusionment-gartner/725033/">GenerativeAI on the Gartner HypeCycle - Trough of disillusionment</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Enthusiasm for generative AI shows signs of cooling</p>
</li>
<li>
<p>In Gartner‚Äôs annual Hype Cycle for Emerging Technologies report, the research and advisory company placed generative AI past the peak of inflated expectations, and down the path towards what it calls the <strong>trough of disillusionment</strong>.</p>
</li>
<li>
<p>Unhappiness with the technology ‚Äî likely stems from three areas:</p>
<div class="ulist">
<ul>
<li>
<p>Current models are versatile but mainly general purpose, and enterprises have struggled to steer them into enterprise use cases.</p>
</li>
<li>
<p>Organizations have underestimated the challenge of setting up governance and data infrastructure for these capabilities.</p>
</li>
<li>
<p>The initial wave of generative AI solutions, while valuable, may not be delivering the high promise vendors claimed.</p>
</li>
</ul>
</div>
</li>
<li>
<p>‚ÄúIt would be a loss if the short-term disillusionment results in enterprises completely pulling away from AI‚Äù</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-07-29-gartner-predicts-30-of-generative-ai-projects-will-be-abandoned-after-proof-of-concept-by-end-of-2025"><a class="anchor" href="#2024-07-29-gartner-predicts-30-of-generative-ai-projects-will-be-abandoned-after-proof-of-concept-by-end-of-2025"></a><a class="link" href="#2024-07-29-gartner-predicts-30-of-generative-ai-projects-will-be-abandoned-after-proof-of-concept-by-end-of-2025">2024-07-29 <a href="https://www.gartner.com/en/newsroom/press-releases/2024-07-29-gartner-predicts-30-percent-of-generative-ai-projects-will-be-abandoned-after-proof-of-concept-by-end-of-2025">Gartner Predicts 30% of Generative AI Projects Will Be Abandoned After Proof of Concept By End of 2025</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>At least 30% of generative AI (GenAI) projects will be abandoned after proof of concept by the end of 2025, due to poor data quality, inadequate risk controls, escalating costs or unclear business value</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-07-25-ai-trained-on-ai-churns-out-gibberish-garbage"><a class="anchor" href="#2024-07-25-ai-trained-on-ai-churns-out-gibberish-garbage"></a><a class="link" href="#2024-07-25-ai-trained-on-ai-churns-out-gibberish-garbage">2024-07-25 <a href="https://www.popsci.com/technology/ai-trained-on-ai-gibberish/">AI trained on AI churns out gibberish garbage</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>new research suggests that cannibalizing of past model outputs would quickly result in strings of babbling AI gibberish and could eventually lead to what‚Äôs being called ‚Äúmodel collapse.‚Äù</p>
</li>
<li>
<p>Over time and successive generations [&#8230;&#8203;][the] model ‚Äúbecomes poisoned with its own projection of reality.‚Äù</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-07-03-googles-emissions-shot-up-48-over-five-years-due-to-ai"><a class="anchor" href="#2024-07-03-googles-emissions-shot-up-48-over-five-years-due-to-ai"></a><a class="link" href="#2024-07-03-googles-emissions-shot-up-48-over-five-years-due-to-ai">2024-07-03 <a href="https://www.datacenterknowledge.com/sustainability/google-s-emissions-shot-up-48-over-five-years-due-to-ai">Google‚Äôs Emissions Shot Up 48% Over Five Years Due to AI</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>According to a new environmental report from [Google]</p>
</li>
<li>
<p>[The] emissions climbed by almost half over five years</p>
</li>
<li>
<p>[It&#8217;ll be hard] to meet [their] goal of eliminating carbon emissions by 2030</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-06-29-ai-drive-brings-microsofts-green-moonshot-down-to-earth-in-west-london"><a class="anchor" href="#2024-06-29-ai-drive-brings-microsofts-green-moonshot-down-to-earth-in-west-london"></a><a class="link" href="#2024-06-29-ai-drive-brings-microsofts-green-moonshot-down-to-earth-in-west-london">2024-06-29 <a href="https://www.theguardian.com/business/article/2024/jun/29/ai-drive-brings-microsofts-green-moonshot-down-to-earth-in-west-london">AI drive brings Microsoft‚Äôs ‚Äògreen moonshot‚Äô down to earth in west London</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[AI] ambition is jarring with its target of being carbon negative by 2030.</p>
</li>
<li>
<p>the company‚Äôs scope 3 emissions ‚Äì such as CO2 related to the materials in its buildings and the electricity people consume when using products such as Xbox ‚Äì are <strong>more than 30% above</strong> their 2020 level.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-06-29-goldman-sachs-on-gen-ai-too-much-spend-too-little-benefit"><a class="anchor" href="#2024-06-29-goldman-sachs-on-gen-ai-too-much-spend-too-little-benefit"></a><a class="link" href="#2024-06-29-goldman-sachs-on-gen-ai-too-much-spend-too-little-benefit">2024-06-29 <a href="https://www.goldmansachs.com/images/migrated/insights/pages/gs-research/gen-ai&#8212;&#8203;too-much-spend%2C-too-little-benefit-/TOM_AI%202.0_ForRedaction.pdf">Goldman Sachs on Gen Ai: Too much spend, too little benefit?</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Tech giants and beyond are set to spend over $1tn on AI capex in coming years, with so far little to show for it.</p>
</li>
<li>
<p>AI‚Äôs ‚Äúkiller application‚Äù has yet to emerge</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-06-21-claude-3-5-sonnet"><a class="anchor" href="#2024-06-21-claude-3-5-sonnet"></a><a class="link" href="#2024-06-21-claude-3-5-sonnet">2024-06-21 <a href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude 3.5 Sonnet</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The updated Claude 3.5 Sonnet shows wide-ranging improvements on industry benchmarks, with particularly strong gains in <strong>agentic coding</strong> and tool use tasks.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-06-08-chatgpt-is-bullshit"><a class="anchor" href="#2024-06-08-chatgpt-is-bullshit"></a><a class="link" href="#2024-06-08-chatgpt-is-bullshit">2024-06-08 <a href="https://link.springer.com/article/10.1007/s10676-024-09775-5">ChatGPT is bullshit</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[LLMs] have been plagued by persistent inaccuracies in their output; these are often called ‚ÄúAI hallucinations‚Äù.</p>
</li>
<li>
<p>We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005)</p>
</li>
<li>
<p>these programs cannot themselves be concerned with truth, and because they are designed to produce text that looks truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.</p>
</li>
<li>
<p>We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.</p>
</li>
<li>
<p>Currently, false statements by ChatGPT and other large language models are described as ‚Äúhallucinations‚Äù, which give policymakers and the public the idea that these systems are misrepresenting the world, and describing what they ‚Äúsee‚Äù.</p>
</li>
<li>
<p>The problem here isn‚Äôt that large language models hallucinate, lie, or misrepresent the world in some way. It‚Äôs that they are not designed to represent the world at all; instead, they are designed to convey convincing lines of text.</p>
</li>
<li>
<p>Solutions such as connecting the LLM to a database don‚Äôt work because, if the models are trained on the database, then the words in the database affect the probability that the chatbot will add one or another word to the line of text it is generating. But this will only make it produce text similar to the text in the database; doing so will make it more likely that it reproduces the information in the database but by no means ensures that it will.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-05-13-hello-gpt-4o"><a class="anchor" href="#2024-05-13-hello-gpt-4o"></a><a class="link" href="#2024-05-13-hello-gpt-4o">2024-05-13 <a href="https://openai.com/index/hello-gpt-4o/">Hello GPT-4o</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>GPT‚Äë4o (‚Äúo‚Äù for ‚Äúomni‚Äù) is a step towards much more natural human-computer interaction‚Äîit accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-05-01-workbench-a-benchmark-dataset-for-agents-in-a-realistic-workplace-setting"><a class="anchor" href="#2024-05-01-workbench-a-benchmark-dataset-for-agents-in-a-realistic-workplace-setting"></a><a class="link" href="#2024-05-01-workbench-a-benchmark-dataset-for-agents-in-a-realistic-workplace-setting">2024-05-01 <a href="https://arxiv.org/abs/2405.00823">WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>We introduce WorkBench: a benchmark dataset for evaluating agents‚Äô ability to execute tasks in a workplace setting.</p>
</li>
<li>
<p>WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks.</p>
<div class="ulist">
<ul>
<li>
<p>These tasks represent common business activities, such as sending emails and scheduling meetings.</p>
</li>
<li>
<p>a task is sent to the agent, which has access to toolkits in various domains. The agent takes actions using these tools, which may alter the sandbox databases. The agent observes the result of using the tool to determine if more actions are required.</p>
</li>
<li>
<p>[One Limitation of study:] While our tasks require multiple actions, they are limited to single-turn chat. [&#8230;&#8203;] a multi-turn chat setup may be more representative of real tasks and could build upon our work.</p>
</li>
</ul>
</div>
</li>
<li>
<p>We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4).</p>
</li>
<li>
<p>We further find that agents‚Äô errors can result in the wrong action being taken, such as an email being sent to the wrong person.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-04-14-sam-altman-we-have-no-idea-how-we-may-one-day-generate-revenue"><a class="anchor" href="#2024-04-14-sam-altman-we-have-no-idea-how-we-may-one-day-generate-revenue"></a><a class="link" href="#2024-04-14-sam-altman-we-have-no-idea-how-we-may-one-day-generate-revenue">2024-04-14 <a href="https://mastodon.social/@nixCraft/112269408187496933">Sam Altman, We have no idea how we may one day generate revenue</a></a></h2>
<div class="sectionbody">
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>We have no current plans to make revenue. We have no idea how we may one day generate revenue. We have made a soft promise to investors that once we build this generally intelligent system, basically we will ask it to figure out an investment return for you.</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Sam Altman - CEO of OpenAI
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-04-06-ny-times-how-tech-giants-cut-corners-to-harvest-data-for-a-i"><a class="anchor" href="#2024-04-06-ny-times-how-tech-giants-cut-corners-to-harvest-data-for-a-i"></a><a class="link" href="#2024-04-06-ny-times-how-tech-giants-cut-corners-to-harvest-data-for-a-i">2024-04-06 <a href="https://archive.ph/2BYtu">NY Times: How Tech Giants Cut Corners to Harvest Data for A.I.</a></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Big Tech has no more sources of data to tap, for their scaling ideas.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In late 2021, OpenAI faced a <strong>supply problem</strong>.</p>
<div class="ulist">
<ul>
<li>
<p>It needed more data to train the next version of its technology ‚Äî lots more. So OpenAI researchers created a speech recognition tool called Whisper. It could transcribe the audio from YouTube videos&#8230;&#8203;</p>
</li>
<li>
<p>But YouTube prohibits people from not only using its videos for ‚Äúindependent‚Äù applications, but also accessing its videos by ‚Äúany automated means (such as robots, botnets or scrapers).‚Äù</p>
</li>
<li>
<p>Ultimately, an OpenAI team transcribed more than one million hours of YouTube videos,</p>
</li>
</ul>
</div>
</li>
<li>
<p>Meta</p>
<div class="ulist">
<ul>
<li>
<p>But by early [2023], Meta had hit the same hurdle as its rivals: not enough data.</p>
</li>
<li>
<p>Meta‚Äôs vice president of generative A.I., told executives that his team had used almost every available English-language book, essay, poem and news article on the internet to develop a model</p>
</li>
<li>
<p>Discussed buying the publishing house Simon &amp; Schuster to procure long works</p>
</li>
<li>
<p>They also conferred on gathering copyrighted data from across the internet, even if that meant facing lawsuits. Negotiating licenses [&#8230;&#8203;] would take too long</p>
</li>
</ul>
</div>
</li>
<li>
<p>Google</p>
<div class="ulist">
<ul>
<li>
<p>transcribed YouTube videos to harvest text for its A.I. models. That potentially violated the copyrights to the videos, which belong to their creators.</p>
</li>
<li>
<p>[Google] didn‚Äôt stop OpenAI because [they] had also used transcripts of YouTube videos to train its A.I. models</p>
</li>
<li>
<p>[Their licensing terms also changed allowing them] to tap <strong>publicly available Google Docs</strong></p>
</li>
</ul>
</div>
</li>
<li>
<p>The volume of data is crucial. Leading chatbot systems have learned from pools of digital text spanning as many as three trillion words, or roughly twice the number of words stored in Oxford University‚Äôs Bodleian Library, which has collected manuscripts since 1602.</p>
</li>
<li>
<p>The most prized data, A.I. researchers said, is high-quality information, such as published books and articles, which have been carefully written and edited by professionals.</p>
</li>
<li>
<p>‚ÄúThe data needed is so massive that even collective licensing really can‚Äôt work.‚Äù</p>
</li>
<li>
<p>‚ÄúScale is all you need‚Äù</p>
</li>
<li>
<p>Synthetic data</p>
<div class="ulist">
<ul>
<li>
<p>[aka] text generated by A.I.</p>
</li>
<li>
<p>‚ÄúAs long as you can get over the synthetic data event horizon, where the model is smart enough to make good synthetic data, everything will be fine,‚Äù</p>
</li>
<li>
<p>Easier said than done. [they] can get caught in a loop where they reinforce their own quirks, mistakes and limitations.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-03-04-httpswww-anthropic-comnewsclaude-3-family"><a class="anchor" href="#2024-03-04-httpswww-anthropic-comnewsclaude-3-family"></a><a class="link" href="#2024-03-04-httpswww-anthropic-comnewsclaude-3-family">2024-03-04 <a href="https://www.anthropic.com/news/claude-3-family">https://www.anthropic.com/news/claude-3-family</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The [Claude 3] family includes three state-of-the-art models in <strong>ascending</strong> order of capability:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Claude 3 Haiku</p>
</li>
<li>
<p>Claude 3 Sonnet</p>
</li>
<li>
<p>Claude 3 Opus</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2024-02-12-careless-whisper-speech-to-text-hallucination-harms"><a class="anchor" href="#2024-02-12-careless-whisper-speech-to-text-hallucination-harms"></a><a class="link" href="#2024-02-12-careless-whisper-speech-to-text-hallucination-harms">2024-02-12 <a href="https://arxiv.org/abs/2402.08021">Careless Whisper: Speech-to-Text Hallucination Harms</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>We evaluate Open AI&#8217;s Whisper [&#8230;&#8203;] we find that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences which did not exist in any form in the underlying audio [&#8230;&#8203; and of those] 38% of hallucinations include explicit harms.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-10-06-google-bard-is-relaunched-as-gemini"><a class="anchor" href="#2023-10-06-google-bard-is-relaunched-as-gemini"></a><a class="link" href="#2023-10-06-google-bard-is-relaunched-as-gemini">2023-10-06 <a href="https://en.wikipedia.org/wiki/Gemini_(chatbot)">Google Bard is relaunched as Gemini</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>the company&#8217;s "largest and most capable AI model"</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-10-09-microsoft-reportedly-is-losing-lots-of-money-per-user-on-github-copilot"><a class="anchor" href="#2023-10-09-microsoft-reportedly-is-losing-lots-of-money-per-user-on-github-copilot"></a><a class="link" href="#2023-10-09-microsoft-reportedly-is-losing-lots-of-money-per-user-on-github-copilot">2023-10-09 <a href="https://www.neowin.net/news/microsoft-reportedly-is-losing-lots-of-money-per-user-on-github-copilot/">Microsoft reportedly is losing lots of money per user on GitHub Copilot</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>[Github Copilot] is available now for $10 a month or $100 for a year&#8217;s subscription.</p>
</li>
<li>
<p>In the first few months of this year, [Microsoft] was <strong>losing n average more than $20 a month</strong> per user, according to a person familiar with the figures, who said some users were costing [Microsoft] as much as <strong>$80 a month</strong>.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-09-dall-e-3-revealed"><a class="anchor" href="#2023-09-dall-e-3-revealed"></a><a class="link" href="#2023-09-dall-e-3-revealed">2023-09 <a href="https://en.wikipedia.org/wiki/DALL-E">DALL-E 3 revealed</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>capable of understanding "significantly more nuance and detail" than previous iterations.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-06-19-google-warns-its-own-employees-do-not-use-code-generated-by-bard"><a class="anchor" href="#2023-06-19-google-warns-its-own-employees-do-not-use-code-generated-by-bard"></a><a class="link" href="#2023-06-19-google-warns-its-own-employees-do-not-use-code-generated-by-bard">2023-06-19 <a href="https://www.theregister.com/2023/06/19/even_google_warns_its_own/">Google warns its own employees: Do not use code generated by Bard</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Google has warned its own employees not to disclose confidential information or use the code generated by its AI chatbot, Bard.</p>
</li>
<li>
<p>Other large firms have similarly cautioned their staff against leaking proprietary documents or code, and have banned them using other AI chatbots.</p>
</li>
<li>
<p>[Google] told Reuters its internal ban was introduced because Bard can output "undesired code suggestions." Issues could potentially lead to buggy programs or complex, bloated software that will cost developers more time to fix than if they didn&#8217;t use AI to code at all.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-05-29-faith-and-fate-limits-of-transformers-on-compositionality"><a class="anchor" href="#2023-05-29-faith-and-fate-limits-of-transformers-on-compositionality"></a><a class="link" href="#2023-05-29-faith-and-fate-limits-of-transformers-on-compositionality">2023-05-29 <a href="https://arxiv.org/abs/2305.18654">Faith and Fate: Limits of Transformers on Compositionality</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The striking discrepancy between the impressive successes of transformer LLMs on seemingly complex tasks and the astonishing failures on seemingly trivial tasks spark critical open questions about how to faithfully interpret their mixed capabilities.</p>
<div class="ulist">
<ul>
<li>
<p>Shortcut learning via pattern-matching may yield fast correct answers when similar compositional patterns are available during training but does not allow for robust generalization to uncommon or complex examples.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Second, due to error propagation, transformers may have inherent limitations on solving high-complexity compositional tasks that exhibit novel patterns.</p>
</li>
<li>
<p>The problems [hallucination, prompt injection, and jailbreaks] are inherent, certainly in the present generation of models and [&#8230;&#8203;] likely in LLMs <em>per se</em></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-04-06-chatgpt-invented-a-sexual-harassment-scandal-and-named-a-real-law-prof-as-the-accused"><a class="anchor" href="#2023-04-06-chatgpt-invented-a-sexual-harassment-scandal-and-named-a-real-law-prof-as-the-accused"></a><a class="link" href="#2023-04-06-chatgpt-invented-a-sexual-harassment-scandal-and-named-a-real-law-prof-as-the-accused">2023-04-06 <a href="https://jonathanturley.org/2023/04/06/defamed-by-chatgpt-my-own-bizarre-experience-with-artificiality-of-artificial-intelligence/">ChatGPT invented a sexual harassment scandal and named a real law prof as the accused</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>I have been writing about the threat of AI to free speech. Then recently I learned that ChatGPT falsely reported on a claim of sexual harassment that was <strong>never made</strong> against me on a trip that <strong>never occurred</strong> while I was on a faculty where I <strong>never taught</strong>. ChapGPT relied on a cited Post article that was <strong>never written</strong> and quotes a statement that was <strong>never made</strong> by the newspaper.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-03-14-cursor-ide-v0-0-37"><a class="anchor" href="#2023-03-14-cursor-ide-v0-0-37"></a><a class="link" href="#2023-03-14-cursor-ide-v0-0-37">2023-03-14 <a href="https://cursor.com/changelog/0-0-37">Cursor IDE v0.0.37</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>First Cursor IDE version</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-03-chatgpt-release"><a class="anchor" href="#2023-03-chatgpt-release"></a><a class="link" href="#2023-03-chatgpt-release">2023-03 <a href="https://en.wikipedia.org/wiki/ChatGPT#Model_versions">ChatGPT release</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Based on GPT 4 (Generative Pre-trained Transformer)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2023-02-24-meta-llama-is-announced"><a class="anchor" href="#2023-02-24-meta-llama-is-announced"></a><a class="link" href="#2023-02-24-meta-llama-is-announced">2023-02-24 <a href="https://en.wikipedia.org/wiki/Llama_(language_model)">Meta LLaMA is announced</a></a></h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="2023-02-06-google-bard-is-announced"><a class="anchor" href="#2023-02-06-google-bard-is-announced"></a><a class="link" href="#2023-02-06-google-bard-is-announced">2023-02-06 <a href="https://en.wikipedia.org/wiki/Gemini_(chatbot)">Google Bard is announced</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Multiple media outlets and financial analysts described Google as "rushing" Bard&#8217;s announcement to preempt rival Microsoft&#8217;s planned February 7 event unveiling its partnership with OpenAI to integrate ChatGPT into its Bing search engine</p>
</li>
<li>
<p>After an "underwhelming" February 8 livestream in Paris showcasing Bard, Google&#8217;s stock fell eight percent, equivalent to a $100 billion loss in market value, and the YouTube video of the livestream was made private.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2022-11-first-chatgpt-release"><a class="anchor" href="#2022-11-first-chatgpt-release"></a><a class="link" href="#2022-11-first-chatgpt-release">2022-11 <a href="https://en.wikipedia.org/wiki/ChatGPT#Model_versions">First ChatGPT release</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Based on GPT 3.5 (Generative Pre-trained Transformer)</p>
</li>
<li>
<p>Gained one million users in five days and 100 millions in two months, becoming the fastest-growing internet application in history.</p>
</li>
</ul>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="2022-06-22-github-copilot-is-now-generally-available-starts-at-10month"><a class="anchor" href="#2022-06-22-github-copilot-is-now-generally-available-starts-at-10month"></a><a class="link" href="#2022-06-22-github-copilot-is-now-generally-available-starts-at-10month">2022-06-22 <a href="https://www.neowin.net/news/github-copilot-is-now-generally-available-starts-at-10month/">GitHub Copilot is now generally available, starts at $10/month</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>More than 1.2 million users enrolled in the preview for GitHub Copilot since June 2021.</p>
</li>
<li>
<p>The program is now available to <strong>all developers for $10/month</strong> and $100/year.</p>
</li>
<li>
<p>Verified students and owners of established open-source projects can keep using it for free.</p>
</li>
<li>
<p>The extension is available on numerous editors such as Visual Studio, Visual Studio Code, Neovim, and JetBrains IDEs.</p>
</li>
<li>
<p>The extension works well with multiple coding languages with notable ones being Python, JavaScript, TypeScript, and Go.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2022-03-10-deep-learning-is-hitting-a-wall"><a class="anchor" href="#2022-03-10-deep-learning-is-hitting-a-wall"></a><a class="link" href="#2022-03-10-deep-learning-is-hitting-a-wall">2022-03-10 <a href="https://archive.ph/6hEYS">Deep Learning Is Hitting a Wall</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Few fields have been more filled with hype and bravado than artificial intelligence.</p>
</li>
<li>
<p>It has flitted from fad to fad decade by decade, always promising the moon, and only occasionally delivering.</p>
</li>
<li>
<p>One minute it was expert systems, next it was Bayesian networks, and then Support Vector Machines.</p>
</li>
<li>
<p>In 2011, it was IBM‚Äôs Watson [&#8230;&#8203;]</p>
</li>
<li>
<p>Nowadays, and in fact ever since 2012, the flavor of choice has been <strong>deep learning</strong> [&#8230;&#8203;].</p>
<div class="ulist">
<ul>
<li>
<p>[The "Godfathers of AI" and "Godfathers of Deep Learning" are Geoffrey Hinton, Yoshua Bengio and Yann LeCun, for which they won the 2018 Turing Award.]</p>
</li>
<li>
<p>[Hinton, the Godfather of AI, joined Google in 2013 when his company was acquired but left May 2023 because he wanted to "freely speak out about the risks of A.I.". He&#8217;s been cited half-a-million times]</p>
</li>
<li>
<p>[Yoshua Bengio is the most-cited computer scientist globally and the most-cited living scientist across all fields]</p>
</li>
<li>
<p>[Yann LeCun, Chief AI Scientist at Meta]</p>
</li>
</ul>
</div>
</li>
<li>
<p>Deep learning, which is fundamentally a technique for recognizing patterns, is at its best when all we need are rough-ready results, where stakes are low and perfect results optional.</p>
</li>
<li>
<p>When a single error can cost a life, it‚Äôs just not good enough.</p>
</li>
<li>
<p>Deep-learning systems are particularly problematic when it comes to ‚Äúoutliers‚Äù that differ substantially from the things on which they are trained.</p>
</li>
<li>
<p>Current deep-learning systems frequently succumb to stupid errors like [the following]. They sometimes misread dirt on an image that a human radiologist would recognize as a glitch.</p>
</li>
<li>
<p>What else might we need? Among other things, we are very likely going to need to revisit a once-popular idea [&#8230;&#8203;]: the idea of manipulating symbols‚Äîcomputer-internal encodings, like strings of binary bits, that stand for complex ideas.</p>
</li>
<li>
<p>What does ‚Äúmanipulating symbols‚Äù really mean? Ultimately, it means two things: having sets of symbols (essentially just patterns that stand for things) to represent information, and processing (manipulating) those symbols in a specific way, using something like algebra (or logic, or computer programs) to operate over those symbols.</p>
</li>
<li>
<p>Classical computer science [of the sort practiced by Turing and von Neumann and everyone after, manipulates symbols in a fashion that we think of as algebraic, and that‚Äôs what‚Äôs really at stake. In simple algebra, we have three kinds of entities, variables (like x and y), operations (like + or -), and bindings (which tell us, for example, to let x = 12 for the purpose of some calculation).</p>
</li>
<li>
<p>If symbols are so critical for software engineering, why not use them in AI, too?</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2022-04-06-dall-e-2-revealed"><a class="anchor" href="#2022-04-06-dall-e-2-revealed"></a><a class="link" href="#2022-04-06-dall-e-2-revealed">2022-04-06 <a href="https://en.wikipedia.org/wiki/DALL-E">DALL-E 2 revealed</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>designed to generate more realistic images at higher resolutions that "can combine concepts, attributes, and styles".</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2021-01-05-dall-e-1-revealed"><a class="anchor" href="#2021-01-05-dall-e-1-revealed"></a><a class="link" href="#2021-01-05-dall-e-1-revealed">2021-01-05 <a href="https://en.wikipedia.org/wiki/DALL-E">DALL-E 1 revealed</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>uses a version of GPT-3 modified to generate images.</p>
</li>
<li>
<p>The software&#8217;s name is a portmanteau of the names of animated robot Pixar character WALL-E and the Catalan surrealist artist Salvador Dal√≠.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="2020-05-22-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks"><a class="anchor" href="#2020-05-22-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks"></a><a class="link" href="#2020-05-22-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks">2020-05-22 <a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG)&#8201;&#8212;&#8201;models which combine pre-trained parametric and non-parametric memory for language generation.</p>
</li>
<li>
<p>For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.</p>
</li>
</ul>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="2017-06-12-attention-is-all-you-need"><a class="anchor" href="#2017-06-12-attention-is-all-you-need"></a><a class="link" href="#2017-06-12-attention-is-all-you-need">2017-06-12 <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A Google paper that lays the foundation upon which all generative AI tools are based on.</p>
</div>
</div>
</div>
</div><!-- Add backlinks to the current page --></main><div id="copyright">
    <p id="copyright-notice">¬© Richard Gro√ü
         - with help from <a href="https://jekyllrb.com/">Jekyll</a>
        and <a href="https://github.com/raghuveerdotnet/simply-jekyll">Simply Jekyll Theme</a></p>
</div> </div>
        <button class="scroll-to-top" id="scroll-to-top"><i class="fa fa-chevron-up"></i></button>
    </div>
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        document.getElementById("scroll-to-top").addEventListener("click", function() {
            window.scrollTo({top: 0, left: 0, behavior: 'smooth'});
        });
    </script></body>
</html>
