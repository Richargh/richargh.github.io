---
title: Generative AI Track record
permalink: /posts/gen-ai-track-record/
tags: genai ai llm
comments: false
status: ongoing
---

//https://poloclub.github.io/transformer-explainer/

== General LLM comparisons

- link:https://arcprize.org/leaderboard[ARC-AGI Leaderboard], shows cost vs score
- link:https://artificialanalysis.ai/[Artificial Analysis] of AI models and API providers
- link:https://www.swebench.com/#verified[SWE-bench], Can Language Models Resolve Real-World GitHub Issues?

// McDonalds order errors
// NY legal errors

// == link:https://www.arxiv.org/pdf/2505.10066[Dark LLMs: The Growing Threat of Unaligned AI Models]

== 2025-06-21 link:https://www.anthropic.com/research/agentic-misalignment[Agentic Misalignment: How LLMs could be insider threats]

* We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm.
* In the scenarios, we allowed models to autonomously send emails and access sensitive information.
* we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction.
* In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals—including blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment.

== 2025-06-10 link:https://www.theguardian.com/commentisfree/2025/jun/10/billion-dollar-ai-puzzle-break-down[When billion-dollar AIs break down over puzzles a child can do, it’s time to rethink the hype - Gary Marcus]

* neural networks of various kinds can generalise within a distribution of data they are exposed to, but their generalisations tend to break down beyond that distribution.
** A simple example of this is that I once trained an older model to solve a very basic mathematical equation using only even-numbered training data. The model was able to generalise a little bit: solve for even numbers it hadn’t seen before, but unable to do so for problems where the answer was an odd number.

== 2025-06-06 link:https://machinelearning.apple.com/research/illusion-of-thinking[The Illusion of Thinking - Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity]

* Recent generations of frontier language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers
* Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities.
* [...] these models fail to develop generalizable problem-solving capabilities for planning tasks, [...]
* At low complexity, non-thinking models are more accurate and token-efficient. As complexity increases, reasoning models outperform but require more tokens—until both collapse beyond a critical threshold, with shorter traces.
* Rather than standard benchmarks (e.g., math problems), we adopt controllable puzzle environments that let us vary complexity systematically—by adjusting puzzle elements while preserving the core logic

== 2025-06-05 link:https://github.com/r-three/common-pile/blob/main/paper.pdf[The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text]

* Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns.
** Recent estimates suggest that compensating the authors of pre-training data, even at conservatively low wage rates, would cost billions of US dollars
* Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs.
* To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining.
** A critical stage of large language model (LLM) development is pretraining, where an LLM is trained to predict the next token (i.e., word or subword unit) in a corpus of unstructured text.
** Pretraining is widely regarded as the foundation for strong downstream performance
** the Common Pile v0.1 focuses primarily on English content
* Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively.
* Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B.
* In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.

== 2025-05-24 link:https://arxiv.org/abs/2505.18878[CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions]

* While AI agents have transformative potential in business, the absence of publicly-available business data on widely used platforms hinders effective performance benchmarking.
* [...] we introduce CRMArena-Pro, a novel benchmark for holistic and realistic assessment of LLM agents in diverse professional settings. [It features] nineteen expert-validated tasks across customer sales, service, as well as configure, price, and quote for Business-to-Business and Business- to-Customer scenarios.
* It also incorporates multi-turn interactions guided by diverse personas and confidentiality awareness assessments.
** we enable[multi-turn interactions] using LLM-powered simulated users. Each simulated user adopts a randomly sampled persona (e.g., You are quality-focused, maintaining high standards in all work) to introduce realistic variability in interaction styles. Critically, these simulated users release task-relevant information incrementally, often initially incomplete, compelling agents to engage in multi-turn dialogue and ask follow-up questions to successfully complete their objectives
* Experiments show leading LLM agents achieve approximately solely 58% single-turn success rate on CRMArena-Pro, with significant performance drops in multi-turn settings to 35%.
* Workflow Execution is notably more tractable, with top-performing agents surpassing 83% success rate in single-turn tasks, while other skills present greater challenges.
* Agents exhibit near-zero inherent confidentiality awareness (improvable with prompting but often at a cost to task performance).

== 2025-05-19 link:https://arxiv.org/pdf/2505.13076[The Hidden Dangers of Browsing AI Agents]

* AI browsing or web agents are autonomous systems that use Large Language Models (LLMs) to navigate and interact with websites on behalf of a user. They typically perceive web content (through page text or visual renderings) and perform actions such as clicking links, filling forms, or entering text, in order to accomplish user-specified tasks. Unlike a standard chatbot, which only produces textual responses, a web agent operates
in an iterative sense-plan-act loop.
* Our work outlines the first end-to-end threat model for browsing agents and provides actionable guidance for securing their deployment in real-world environments.
* To address discovered threats, we propose a defense-in-depth strategy incorporating input sanitization, planner-executor isolation, formal analyzers, and session safeguards—providing protection against both initial access and post-exploitation attack vectors.
* Mitigation
** Defending Against Initial Access Attack Vectors
*** Input Sanitization and Encapsulation (f.ex. markers around user prompt; rewrite or filter the prompt; sandwiching - a safe guard instruction after tool outputs)
*** Automatic Paraphrasing (f.ex. reordering steps or changing words)
*** LLM-Based Detection (f.ex. secondary LLM, fine-tuned on typical injections)
*** Robust Prompting & Fine-Tuning (f.ex. system prompts that teach the model to treat certain content as nonexecutable data)
*** Architectural Isolation – Planner (strictly trusted inputs) vs. Executor (performs actions on all data, including untrusted content). This way untrusted content cannot derail future planner actions.
*** Formal Security Analyzers: Before the agent executes any tool, the analyzer checks the proposed action against these rules and blocks it if it violates a policy, such as triggered by untrusted content
** Defending Against Post-Exploitation Attack Vectors
*** Agent State Reset (Session Isolation): agent resets if attack detected or suspected
*** Information Flow Control Policies: By defining “sources” (sensitive data locations) and “sinks” (potential exfiltration channels), the agent can automatically block or require approval for risky combinations of actions.
*** LLM-Based Memory Inspection: an attacker might plant secrets in memory to be leaked later. Perplexity-based scanning checks if the memory contains unusually predictable (likely compromised) text.
*** Activity Audit and Throttling: monitor agent actions for anomalies
*** Fallback to Safe Mode: In safe mode, only a minimal set of read-only actions are allowed,
*** Red Team and Patching Cycle: patch the agent against exploits to harden it over time

== 2025-05-13 link:https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5219933[Large Language Models, Small Labor Market Effects]

* examine the labor market effects of AI chatbots using two large-scale adoption surveys (late 2023 and 2024) covering 11 exposed occupations (25,000 workers, 7,000 workplaces)
* despite substantial investments, economic impacts remain minimal
* [...] we estimate precise zeros: AI chatbots have had no significant impact on earnings or recorded hours in any occupation [...]
* Modest productivity gains (average time savings of 3%), combined with weak wage pass-through, help explain these limited labor market effects.
* Our findings challenge narratives of imminent labor market transformation due to Generative AI.
* two years after the fastest technology adoption ever, labor market outcomes—whether at the individual or firm level—remain untouched.

== 2025-04-26 link:https://www.msn.com/en-us/news/technology/we-now-know-how-ai-thinks-and-it-s-barely-thinking-at-all/ar-AA1DDDZv[We Now Know How AI ‘Thinks’—and It’s Barely Thinking at All - The Wall Street Journal]

* All of this work suggests that under the hood, today’s AIs are overly complicated, patched-together Rube Goldberg machines full of ad-hoc solutions for answering our prompts.
* Understanding that these systems are long lists of cobbled-together rules of thumb could go a long way to explaining why they struggle when they’re asked to do things even a little bit outside their training [...]
* [A model trained on millions of turn-by-turn directions in Manhattan] managed to give usable turn-by-turn directions between any two points in the borough with 99% accuracy. [...] [But when the researches] blocked just 1% of the virtual Manhattan’s roads, forcing the AI to navigate around detours, its performance plummeted.
* [The] research also suggests why many models are so massive: They have to memorize an endless list of rules of thumb, and can’t compress that knowledge into a mental model like a person can.

// == 2025-04-22 link:https://arxiv.org/abs/2504.15681[Vidi: Large Multimodal Models for Video Understanding and Editing]

== 2025-04-14 link:https://arxiv.org/abs/2504.09762v2[Stop Anthropomorphizing Intermediate Tokens as Reasoning/Thinking Traces!]

* Intermediate token generation (ITG), where a model produces output before the solution, has been proposed as a method to improve the performance of language models on reasoning tasks.
* These intermediate tokens have been called "reasoning traces" or even "thoughts" -- implicitly anthropomorphizing the model, implying these tokens resemble steps a human might take
* Recent advances in general planning and problem solving have been spearheaded by so-called “Long Chain-of-Thought” models, most notably DeepSeek’s R1
* In this paper, we take the position that anthropomorphizing intermediate tokens as reasoning/thinking traces is (1) wishful (2) has little concrete supporting evidence (3) engenders false confidence and(4) may be pushing the community into fruitless research directions.
* Anthropomorphization of the intermediate tokens as reasoning/thinking traces has provided a comforting explanation of the observed performance of LRMs.Our arguments in this paper foreground the possibility that this is a cargo cult explanation [ 11 ], namely that derivation traces resemble reasoning in syntax only.


== 2025-03-13 link:https://arstechnica.com/ai/2025/03/ai-search-engines-give-incorrect-answers-at-an-alarming-60-rate-study-says/[AI search engines cite incorrect news sources at an alarming 60% rate, study says]

* They discovered that the AI models incorrectly cited sources in more than 60 percent of these queries.
** Perplexity provided incorrect information in 37 percent of the queries tested,
** whereas ChatGPT Search incorrectly identified 67 percent (134 out of 200) of articles queried.
** Grok 3 demonstrated the highest error rate, at 94 percent.
* In total, researchers ran 1,600 queries across the eight different generative search tools.
* Surprisingly, premium paid versions of these AI search tools fared even worse in certain respects. Though these premium models correctly answered a higher number of prompts, their reluctance to decline uncertain responses drove higher overall error rates.
** Perplexity Pro ($20/month) and Grok 3's premium service ($40/month) confidently delivered incorrect responses more often than their free counterparts.
* On some occasions, the chatbots either incorrectly answered or declined to answer queries from publishers that permitted them to access their content. On the other hand, they sometimes correctlyanswered queries about publishers whose content they shouldn’t have had access to

== 2025-03-06 link:https://www.cjr.org/tow_center/we-compared-eight-ai-search-engines-theyre-all-bad-at-citing-news.php[AI Search Has A Citation Problem]

- Chatbots were generally bad at declining to answer questions they couldn’t answer accurately, offering incorrect or speculative answers instead.
- Premium chatbots provided more confidently incorrect answers than their free counterparts.
- Multiple chatbots seemed to bypass Robot Exclusion Protocol preferences.
- Generative search tools fabricated links and cited syndicated and copied versions of articles.
- Content licensing deals with news sources provided no guarantee of accurate citation in chatbot responses.

== 2025-02-26 link:https://arxiv.org/abs/2503.05777[Medical Hallucinations in Foundation Models and Their Impact on Healthcare]

* [...] a key limitation of their reliability is hallucination, where inaccurate or fabricated information can impact clinical decisions and patient safety.
* Our results reveal that inference techniques such as Chain-of-Thought (CoT) and Search Augmented Generation can effectively reduce hallucination rates. However, despite these improvements, non-trivial levels of hallucination persist.

== 2025-02-06 link:https://arstechnica.com/tech-policy/2025/02/meta-torrented-over-81-7tb-of-pirated-books-to-train-ai-authors-say/[”Torrenting from a corporate laptop doesn’t feel right”: Meta emails unsealed]
* Last month, Meta admitted to torrenting a controversial large dataset known as LibGen, which includes tens of millions of pirated books

== 2025-02-03 link:https://www.404media.co/anthropic-claude-job-application-ai-assistants/[AI Company Asks Job Applicants Not to Use AI in Job Applications]
* Anthropic, the developer of the conversational AI assistant Claude, doesn’t want prospective new hires using AI assistants in their applications, regardless of whether they’re in marketing or engineering.
* “While we encourage people to use AI systems during their role to help them work faster and more effectively, please do not use AI assistants during the application process,”

== 2025-01-20 link:https://queue.acm.org/detail.cfm?id=3711679[The Price of Intelligence - Three risks inherent in LLMs]

* Discussions of LLM capabilities often overlook their inherently probabilistic nature [...]
** [The models are losing data. They are trained] with billions of parameters on trillions of tokens, making it impossible for a model to perfectly memorize all information in its training data.
** The generation process is also stochastic.
* These characteristics give rise to three intrinsic behaviors:
** Hallucination
** Indirect prompt injection [e.g. E-Mails that are passed to the LLM, where the contents derail or even change the intended user prompt]
** Jailbreaks, [crafted input prompts] bypassing built-in safeguards or ethical guidelines
* These behaviors pose significant challenges for the widespread adoption of LLMs, particularly in high-stakes domains such as healthcare, finance, or legal applications.
* We argue that there is no simple "fix" for these behaviors, but they are instead fundamental to how these models operate.

== 2025-01-03 link:https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2025/01/ai-risk-consumer-harm[AI and the Risk of Consumer Harm]
* The FTC is increasingly taking note of AI’s potential for and real-world instances of harm
** from incentivizing commercial surveillance
** to enabling fraud and impersonation
** to perpetuating illegal discrimination
* companies [should] consider these factors when developing, maintaining, using, and deploying an AI-based product:
** Taking necessary steps to prevent harm before and after deploying a product.
** Taking preventative measures to detect, deter, and halt AI-related impersonation, fraud, child sexual abuse material, and non-consensual intimate imagery.
** Avoiding deceptive claims about AI tools that result in people losing money or put users at risk of harm.
** Ensuring privacy and security by default.

== 2024-12-13 link:https://arxiv.org/abs/2412.09871?trk=public_post_reshare-text[Byte Latent Transformer: Patches Scale Better Than Tokens]
* The Byte Latent Transformer (BLT), is a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness

== 2024-11-27 link:https://www.theverge.com/2024/11/27/24307284/microsoft-debunks-office-ai-data-scraping-rumors[Microsoft says it isn’t using M360 data to train AI models]
* Microsoft says it isn’t using customer data from its Microsoft 365 apps to train its AI models.
* The confusion arose from a privacy setting in Microsoft Office that toggles “optional connected experiences”

== 2024-11-21 link:https://www.businessinsider.com/microsoft-copilot-oversharing-problem-fix-customers-2024-11[Microsoft Copilot shares sensitive information, ignoring rights]
* A [Microsoft] Copilot security issue that inadvertently let employees access sensitive information such as CEO emails and HR documents.
* Microsoft Copilot and Github Copilot are different services. The first one is integrated into M365, the latter into IDEs to generate code.

== 2024-11-13 link:https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai[OpenAI, Google and Anthropic are struggling to build more advanced AI]
* [OpenAis new Model] Orion fell short when trying to answer coding questions that it hadn’t been trained on
* An upcoming iteration of [Google's] Gemini software is not living up to internal expectations
* Anthropic, meanwhile, has seen the timetable slip for the release of its long-awaited Claude model called 3.5 Opus.
* The companies are facing several challenges.
** It’s become increasingly difficult to find new, untapped sources of high-quality, human-made training data that can be used to build more advanced AI systems.
** Even modest improvements may not be enough to justify the tremendous costs associated with building and operating new models
* “We got very excited for a brief period of very fast progress, That just wasn’t sustainable.”
* Like Google and Anthropic, OpenAI is now shifting attention from the size of these models to newer use cases, including a crop of AI tools called agents that can book flights or send emails on a user’s behalf.

== 2024-10-21 link:https://www.ciodive.com/news/gartner-symposium-keynote-AI/730486/[Gartner sounds alarm on AI cost, data challenges]
* CIOs are still in search of the generative AI sweet spot where workflows are enhanced, but costs and risks are manageable
* Nearly half of CIOs say AI has not yet met ROI expectations, according to Gartner research.
* “The truth is that you’ve been in the mud for the last year, working hard to find all those benefits that were promised by AI,”
* Part of the disillusionment business leaders are feeling comes from the immaturity of the technology and the pace of innovation.
* “Cost is as big an AI risk as security. With generative AI, it’s really easy to waste money.”
* CIOs could miscalculate AI costs by as much as 1,000% as they scale AI plans, Gartner research suggests.
* “Set aside all that hype and focus on your pace,” LeHong said. “Choose the one that’s right for you and run your own race.”

// 2024-10-07 link:https://arxiv.org/pdf/2410.05229[Understanding the Limitations of Mathematical Reasoning in Large Language Models]

== 2024-09-27 link:https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html[OpenAI Is Growing Fast and Burning Through Piles of Money]
* OpenAI’s monthly revenue hit $300 million in August, up 1,700 percent since the beginning of 2023, and the company expects about *$3.7 billion in annual sales* this year
* Roughly *10 million* ChatGPT users pay the company a *$20 monthly fee*, according to the documents. OpenAI expects to raise that price by $2 by the end of the year, and will aggressively raise it to $44 over the next five years
* It expects to *lose roughly $5 billion* this year after paying for costs related to running its services
* [They are planning] an investment round that could bring in $7 billion and value the company at $150 billion, among the highest ever for a private tech company

== 2024-09-16 link:https://www.cio.com/article/3540579/devs-gaining-little-if-anything-from-ai-coding-assistants.html[CIO: Devs gaining little (if anything) from AI coding assistants]
* Uplevel, using data generated by its customers, compared the output of about 800 developers using GitHub Copilot over a three-month period to their output in a three-month period before adoption.
* The study measured pull request (PR) cycle time, or the time to merge code into a repository, and PR throughput, the number of pull requests merged. It found *no significant improvements* for developers using Copilot.
* Use of GitHub Copilot also introduced *41% more bugs*

//== 2024-09-16 link:https://www.wheresyoured.at/subprimeai/[The Subprime AI Crisis] The AI Bubble implosion

== 2024-09-20 link:https://edition.cnn.com/2024/09/20/energy/three-mile-island-microsoft-ai/index.html[Microsoft revives the nuclear reactor that was responsible for the worst nuclear disaster in US history, to power its AI efforts]
* Three Mile Island, the site of worst nuclear disaster in the United States, is reopening and will exclusively sell the power to Microsoft as the company searches for energy sources to fuel its AI ambitions.
* The Unit 1 reactor, which closed five years ago, is expected to be revived in 2028


// == 2024-09-05 link:https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566[The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers]

== 2024-08-23 link:https://www.ciodive.com/news/generative-ai-hype-moment-reckoning-trough-disillusionment-gartner/725033/[GenerativeAI on the Gartner HypeCycle - Trough of disillusionment]
* Enthusiasm for generative AI shows signs of cooling
* In Gartner’s annual Hype Cycle for Emerging Technologies report, the research and advisory company placed generative AI past the peak of inflated expectations, and down the path towards what it calls the *trough of disillusionment*.
* Unhappiness with the technology — likely stems from three areas:
** Current models are versatile but mainly general purpose, and enterprises have struggled to steer them into enterprise use cases.
** Organizations have underestimated the challenge of setting up governance and data infrastructure for these capabilities.
** The initial wave of generative AI solutions, while valuable, may not be delivering the high promise vendors claimed.
* “It would be a loss if the short-term disillusionment results in enterprises completely pulling away from AI”

== 2024-07-29 link:https://www.gartner.com/en/newsroom/press-releases/2024-07-29-gartner-predicts-30-percent-of-generative-ai-projects-will-be-abandoned-after-proof-of-concept-by-end-of-2025[Gartner Predicts 30% of Generative AI Projects Will Be Abandoned After Proof of Concept By End of 2025]
* At least 30% of generative AI (GenAI) projects will be abandoned after proof of concept by the end of 2025, due to poor data quality, inadequate risk controls, escalating costs or unclear business value

== 2024-07-25 link:https://www.popsci.com/technology/ai-trained-on-ai-gibberish/[AI trained on AI churns out gibberish garbage]

* new research suggests that cannibalizing of past model outputs would quickly result in strings of babbling AI gibberish and could eventually lead to what’s being called “model collapse.”
* Over time and successive generations [...][the] model “becomes poisoned with its own projection of reality.”

== 2024-07-03 link:https://www.datacenterknowledge.com/sustainability/google-s-emissions-shot-up-48-over-five-years-due-to-ai[Google’s Emissions Shot Up 48% Over Five Years Due to AI]
* According to a new environmental report from [Google]
* [The] emissions climbed by almost half over five years
* [It'll be hard] to meet [their] goal of eliminating carbon emissions by 2030

== 2024-06-29 link:https://www.theguardian.com/business/article/2024/jun/29/ai-drive-brings-microsofts-green-moonshot-down-to-earth-in-west-london[AI drive brings Microsoft’s ‘green moonshot’ down to earth in west London]
* [AI] ambition is jarring with its target of being carbon negative by 2030.
* the company’s scope 3 emissions – such as CO2 related to the materials in its buildings and the electricity people consume when using products such as Xbox – are *more than 30% above* their 2020 level.

== 2024-06-29 link:https://www.goldmansachs.com/images/migrated/insights/pages/gs-research/gen-ai--too-much-spend%2C-too-little-benefit-/TOM_AI%202.0_ForRedaction.pdf[Goldman Sachs on Gen Ai: Too much spend, too little benefit?]
* Tech giants and beyond are set to spend over $1tn on AI capex in coming years, with so far little to show for it.
* AI’s “killer application” has yet to emerge

// 2024-05-13 link:https://www.mdpi.com/2076-3417/14/10/4115[The Impact of Large Language Models on Programming Education and Student Learning Outcomes]

== 2024-06-08 link:https://link.springer.com/article/10.1007/s10676-024-09775-5[ChatGPT is bullshit]

* [LLMs] have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”.
* We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005)
* these programs cannot themselves be concerned with truth, and because they are designed to produce text that looks truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.
* We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.
* Currently, false statements by ChatGPT and other large language models are described as “hallucinations”, which give policymakers and the public the idea that these systems are misrepresenting the world, and describing what they “see”.
* The problem here isn’t that large language models hallucinate, lie, or misrepresent the world in some way. It’s that they are not designed to represent the world at all; instead, they are designed to convey convincing lines of text.
* Solutions such as connecting the LLM to a database don’t work because, if the models are trained on the database, then the words in the database affect the probability that the chatbot will add one or another word to the line of text it is generating. But this will only make it produce text similar to the text in the database; doing so will make it more likely that it reproduces the information in the database but by no means ensures that it will.

== 2024-05-01 link:https://arxiv.org/abs/2405.00823[WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting]

* We introduce WorkBench: a benchmark dataset for evaluating agents’ ability to execute tasks in a workplace setting.
* WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks.
** These tasks represent common business activities, such as sending emails and scheduling meetings.
** a task is sent to the agent, which has access to toolkits in various domains. The agent takes actions using these tools, which may alter the sandbox databases. The agent observes the result of using the tool to determine if more actions are required.
** [One Limitation of study:] While our tasks require multiple actions, they are limited to single-turn chat. [...] a multi-turn chat setup may be more representative of real tasks and could build upon our work.
* We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4).
*  We further find that agents’ errors can result in the wrong action being taken, such as an email being sent to the wrong person.


== 2024-04-14 link:https://mastodon.social/@nixCraft/112269408187496933[Sam Altman, We have no idea how we may one day generate revenue]
[quote, Sam Altman - CEO of OpenAI]
____
We have no current plans to make revenue. We have no idea how we may one day generate revenue. We have made a soft promise to investors that once we build this generally intelligent system, basically we will ask it to figure out an investment return for you.
____

== 2024-04-06 link:https://archive.ph/2BYtu[NY Times: How Tech Giants Cut Corners to Harvest Data for A.I.]

Big Tech has no more sources of data to tap, for their scaling ideas.

* In late 2021, OpenAI faced a *supply problem*.
** It needed more data to train the next version of its technology — lots more. So OpenAI researchers created a speech recognition tool called Whisper. It could transcribe the audio from YouTube videos...
** But YouTube prohibits people from not only using its videos for “independent” applications, but also accessing its videos by “any automated means (such as robots, botnets or scrapers).”
** Ultimately, an OpenAI team transcribed more than one million hours of YouTube videos,
* Meta
** But by early [2023], Meta had hit the same hurdle as its rivals: not enough data.
** Meta’s vice president of generative A.I., told executives that his team had used almost every available English-language book, essay, poem and news article on the internet to develop a model
** Discussed buying the publishing house Simon & Schuster to procure long works
** They also conferred on gathering copyrighted data from across the internet, even if that meant facing lawsuits. Negotiating licenses [...] would take too long
* Google
** transcribed YouTube videos to harvest text for its A.I. models. That potentially violated the copyrights to the videos, which belong to their creators.
** [Google] didn’t stop OpenAI because [they] had also used transcripts of YouTube videos to train its A.I. models
** [Their licensing terms also changed allowing them] to tap *publicly available Google Docs*
* The volume of data is crucial. Leading chatbot systems have learned from pools of digital text spanning as many as three trillion words, or roughly twice the number of words stored in Oxford University’s Bodleian Library, which has collected manuscripts since 1602.
* The most prized data, A.I. researchers said, is high-quality information, such as published books and articles, which have been carefully written and edited by professionals.
* “The data needed is so massive that even collective licensing really can’t work.”
* “Scale is all you need”
* Synthetic data
** [aka] text generated by A.I.
** “As long as you can get over the synthetic data event horizon, where the model is smart enough to make good synthetic data, everything will be fine,”
** Easier said than done. [they] can get caught in a loop where they reinforce their own quirks, mistakes and limitations.

== 2024-02-12 link:https://arxiv.org/abs/2402.08021[Careless Whisper: Speech-to-Text Hallucination Harms]
* We evaluate Open AI's Whisper [...] we find that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences which did not exist in any form in the underlying audio [... and of those] 38% of hallucinations include explicit harms.

// 2024-01-09 link:https://codescene.com/hubfs/whitepapers/Refactoring-vs-Refuctoring-Advancing-the-state-of-AI-automated-code-improvements.pdf[Refactoring vs Refuctoring: Advancing the state of AI-automated code improvements]

== 2023-10-06 link:https://en.wikipedia.org/wiki/Gemini_(chatbot)[Google Bard is relaunched as Gemini]
* the company's "largest and most capable AI model"

== 2023-10-09 link:https://www.neowin.net/news/microsoft-reportedly-is-losing-lots-of-money-per-user-on-github-copilot/[Microsoft reportedly is losing lots of money per user on GitHub Copilot]
* [Github Copilot] is available now for $10 a month or $100 for a year's subscription.
* In the first few months of this year, [Microsoft] was *losing n average more than $20 a month* per user, according to a person familiar with the figures, who said some users were costing [Microsoft] as much as *$80 a month*.

== 2023-09 link:https://en.wikipedia.org/wiki/DALL-E[DALL-E 3 revealed]
* capable of understanding "significantly more nuance and detail" than previous iterations.

== 2023-06-19 link:https://www.theregister.com/2023/06/19/even_google_warns_its_own/[Google warns its own employees: Do not use code generated by Bard]
* Google has warned its own employees not to disclose confidential information or use the code generated by its AI chatbot, Bard.
* Other large firms have similarly cautioned their staff against leaking proprietary documents or code, and have banned them using other AI chatbots.
* [Google] told Reuters its internal ban was introduced because Bard can output "undesired code suggestions." Issues could potentially lead to buggy programs or complex, bloated software that will cost developers more time to fix than if they didn't use AI to code at all.

== 2023-05-29 link:https://arxiv.org/abs/2305.18654[Faith and Fate: Limits of Transformers on Compositionality]

* The striking discrepancy between the impressive successes of transformer LLMs on seemingly complex tasks and the astonishing failures on seemingly trivial tasks spark critical open questions about how to faithfully interpret their mixed capabilities.
** Shortcut learning via pattern-matching may yield fast correct answers when similar compositional patterns are available during training but does not allow for robust generalization to uncommon or complex examples.
* Second, due to error propagation, transformers may have inherent limitations on solving high-complexity compositional tasks that exhibit novel patterns.
* The problems [hallucination, prompt injection, and jailbreaks] are inherent, certainly in the present generation of models and [...] likely in LLMs _per se_

== 2023-04-06 link:https://jonathanturley.org/2023/04/06/defamed-by-chatgpt-my-own-bizarre-experience-with-artificiality-of-artificial-intelligence/[ChatGPT invented a sexual harassment scandal and named a real law prof as the accused]
* I have been writing about the threat of AI to free speech. Then recently I learned that ChatGPT falsely reported on a claim of sexual harassment that was *never made* against me on a trip that *never occurred* while I was on a faculty where I *never taught*. ChapGPT relied on a cited Post article that was *never written* and quotes a statement that was *never made* by the newspaper.

== 2023-03 link:https://en.wikipedia.org/wiki/ChatGPT#Model_versions[ChatGPT release]
* Based on GPT 4 (Generative Pre-trained Transformer)

== 2023-02-24 link:https://en.wikipedia.org/wiki/Llama_(language_model)[Meta LLaMA is announced]

== 2023-02-06 link:https://en.wikipedia.org/wiki/Gemini_(chatbot)[Google Bard is announced]
* Multiple media outlets and financial analysts described Google as "rushing" Bard's announcement to preempt rival Microsoft's planned February 7 event unveiling its partnership with OpenAI to integrate ChatGPT into its Bing search engine
* After an "underwhelming" February 8 livestream in Paris showcasing Bard, Google's stock fell eight percent, equivalent to a $100 billion loss in market value, and the YouTube video of the livestream was made private.

== 2022-11 link:https://en.wikipedia.org/wiki/ChatGPT#Model_versions[First ChatGPT release]
* Based on GPT 3.5 (Generative Pre-trained Transformer)
* Gained one million users in five days and 100 millions in two months, becoming the fastest-growing internet application in history.

'''

== 2022-06-22 link:https://www.neowin.net/news/github-copilot-is-now-generally-available-starts-at-10month/[GitHub Copilot is now generally available, starts at $10/month]
* More than 1.2 million users enrolled in the preview for GitHub Copilot since June 2021.
* The program is now available to *all developers for $10/month* and $100/year.
* Verified students and owners of established open-source projects can keep using it for free.
* The extension is available on numerous editors such as Visual Studio, Visual Studio Code, Neovim, and JetBrains IDEs.
* The extension works well with multiple coding languages with notable ones being Python, JavaScript, TypeScript, and Go.

== 2022-04-06 link:https://en.wikipedia.org/wiki/DALL-E[DALL-E 2 revealed]
* designed to generate more realistic images at higher resolutions that "can combine concepts, attributes, and styles".

== 2021-01-05 link:https://en.wikipedia.org/wiki/DALL-E[DALL-E 1 revealed]
* uses a version of GPT-3 modified to generate images.
* The software's name is a portmanteau of the names of animated robot Pixar character WALL-E and the Catalan surrealist artist Salvador Dalí.

== 2020-05-22 link:https://arxiv.org/abs/2005.11401[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks]

* We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation.
* For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.

'''

== 2017-06-12 link:https://arxiv.org/abs/1706.03762[Attention is all you need]
* We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

A Google paper that lays the foundation upon which all generative AI tools are based on.